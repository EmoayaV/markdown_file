## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\4.【2016】【ECCV】【Brain Chu、Hoffman】【UC伯克利】【说明finetune比frozen好，搬过来预训练好的层越多越好】Best practices for fine-tuning Visual classifiers to new domains.pdf)

### 1.1 在干什么、有什么贡献

**这是在[3]的基础上进一步进行实验**

finetune很好，但是对于从预训练网络复制多少层以及是否finetune或frozen这些层，几乎没有指导。是个问题，要去试。

我们基于目标域的两个指标来进行实验调整finetune，这两个指标分别是源域和目标域的距离、目标域训练数据集的多少。

我们总结了最好的finetune方法，并提供以下建议：

* 除了最后一层，尽量把前面的所有层都复制到自己要训练的网络
* finetune所有从预训练网络搬过来的层

![image-20220917163439099](D:\markdown file\截图\image-20220917163439099.png)

这和李沐提到的finetune方法是一致的。

![image-20220918160825164](D:\markdown file\截图\image-20220918160825164.png)

和[3]这篇文章的实验结果一致。



## 2. 实验

### 2.1 如何度量源域的数据集分布和目标域数据集分布差了多少呢？

![image-20220922133904148](D:\markdown file\截图\image-20220922133904148.png)

作者用余弦距离cosine distance和MMD方法来度量两个数据集之间的相似度。最终还是用了余弦距离来度量。越低表明源域和目标域的数据分布越相似。



### 2.2 实验结果

![image-20220922140028136](D:\markdown file\截图\image-20220922140028136.png)

主要看这个表，image per class表示的就是目标数据集的大小，cosine distance表示的就是源域的数据集和目标域数据集的相似程度，越小表示越相似。

随着距离的增加，微调相对于冻结会改善，随着目标域数据集大小的增加，微调效果会越来越好。

看蓝色圈的那一块，在源域的数据集和目标域数据集的相似程度大，目标数据集小，这种情况下freeze比finetune效果好，实际中出现这种情况只可能是从源域的数据集中抽出一小部分作为目标域的数据集这种情况，但是这样做其实没有太大的意义，之所以有迁移学习这种技术就是因为源域的数据和目标域的数据之间相似度不高。

在看红色圈那块，实际中也是红色区域的情况比较常见，即源域的数据集和目标域数据集的相似程度中等，目标数据集中等但比源域数据集小这种情况。

**总之就是开头写的那两点，能finetune就finetune，除了最后一层，能搬多少层搬多少层。**



















