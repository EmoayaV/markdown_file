## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\2.【2014】【CACM】【Goodfellow、Bengio】【GAN原始】Generative Adversarial Nets.pdf)

### 1.1 在干什么，有什么贡献

背景：大多数深度学习模型都用判别模型而不用生成模型，作者将二者结合了。

提出了一个新框架（framework），这个框架有两个模型（model），生成模型（generative model）和判别模型（discriminative model），判别模型D去分辨数据来自原始数据集还是来自模型生成，生成模型G去伪造数据争取骗过判别模型D。

生成模型是一个mlp，输入是一个随机的噪音，mlp能把产生随机噪音的分布（通常时高斯分布）映射到仍和一个我们想去拟合的分布。判别模型一般也用mlp。对于生成模型和判别模型都用mlp的情况，这种网络叫做对抗网络（adversarial nets）。

### 1.2 结论是什么

结论是这个框架确实有用。



## 2. 相关工作

生成模型有两种方式，第一种是说把分布学出来，使得我知道里面的均值方差等等时什么，第二种是说不去学习分布，而是学一个模型来近似你要的结果即可，第二种方式也叫做生成机（generative machines）。



## 3. 对抗网络（Adersarial nets）

### 3.1 网络结构

GAN是一个框架，当生成器和判别器都是MLP时，叫做对抗网络。



其核心思想就是输入一组100维的向量，向量中的每个元素都是一个随机变量，服从均值0方差1的高斯分布，将这组向量输入生成器（一般是MLP）让机器自己去学习如何生成图片，也就是学习一种映射。

### 3.2 损失函数

![image-20220908131523037](D:\markdown file\截图\image-20220908131523037.png)

如何理解这个公式呢，首先x是服从于p~data~(x)这个分布的，z是服从于p~z~(z)这个分布的，通俗来讲就是x是来自于真实的图片，z是一个服从于01高斯分布的多维随机变量，也就是噪音。

对于G(z)来说，就是把多维的噪声放入一个MLP去生成图片，这个图片与真实图片越接近越好，所以要最小化G。对于D(x)来说，我们想让D分辩不出图片来自于G还是真实样本，所以我们要最大化D，因为最好的情况是D认为图片全部来自真实样本，这时D输出的值全是1。

对log项求期望是什么意思呢？其实就是加和求平均的意思，对于一个batch，我们也是这样求一个batch的loss的。

### 3.3 训练过程

![image-20220908133547727](D:\markdown file\截图\image-20220908133547727.png)

如何理解这个图呢，首先这图下面有两根黑实线，一根代表真实样本x一根代表噪声z，x服从真实样本的分布，其采样后的值表现为图上的黑点，z服从高斯分布，其均匀采样后的值经过生成器（也就是MLP）后表现为图上的绿线，箭头表示噪声z经过生成器后映射到了真实样本分布的那一块。蓝线就是判别器判别的结果。

在图上可以看出绿线G(z)和黑线x越拟合，蓝线D的值越大，最后完全拟合时，蓝线的值为1/2.

![image-20220909100657671](D:\markdown file\截图\image-20220909100657671.png)

具体的流程如上图，大概意思就是先固定generator参数，去训练discriminator参数。再固定discriminator参数，去训练generator参数。

K是个超参数，决定D训练的好坏，如果K太大，D训练的太好，那么对于生成器输入的样本，其损失函数很难下降，因为他把生成器输入的全部判为0了。如果K太小，D训练的太差，那么对于生成器输入的样本，其损失函数很容易下降，因为他把生成器输入的全部判为1了，这样对G来说进步太小。

### 3.4 理论推导

![image-20220909123415612](D:\markdown file\截图\image-20220909123415612.png)

* 定理1：当生成器参数固定时，最优的判别器为D*~G~(x) =  p~data~ / p~data~ + p~g~， 当D*~G~(x) =  1/2 证明生成器与真实数据分布相同。
* 定理1的推广 two sample test：判断两块数据是不是来自于同一个分布，可以用一个二分类器来判断，如果二分类器能分开就证明两块数据来自于不同个分布，如果分不开就证明两块数据来自于不同个分布同一个分布。比如在训练集上训练好了一个模型，部署到另外一个环境，要看新的测试数据和训练数据是不是一样的时候，就训练一个分类器把它分一下就行了。

![image-20220909125622327](D:\markdown file\截图\image-20220909125622327.png)

证明：当生成器参数固定时，D(x) = p~data~

/ p~data~ + p~g~ 可以使损失函数达到最大值。

这里设x = g(z)，仅仅是一个变量代换，并不是说前面的x与后面的x相同，前面的x表示由真实样本采样的数据，后面的x表示由生成器生成的样本。



![image-20220909132747379](D:\markdown file\截图\image-20220909132747379.png)

* 定理2：当生成器的数据分布和真实的数据分布相同时，即p~g~ = p~data~损失函数存在最优解。

![image-20220909132808691](D:\markdown file\截图\image-20220909132808691.png)

![image-20220909133047079](D:\markdown file\截图\image-20220909133047079.png)

![image-20220909133642878](D:\markdown file\截图\image-20220909133642878.png)

证明：首先把定理1的结论带入，这时D已经是最优，只用求G的最小值。将式子改写为KL散度的形式，当KL散度取0时，损失函数取得最小值-log(4)，此时p~g~ = p~data~。

KL散度：KL(p||q) = E~x服从于p~(log(p(x) / q(x)))，意思就是我在知道p的情况下，至少要多少比特才能把q描述出来，当P(x) = q(x)时，KL散度取得最小值，两个分布相等。所以KL散度越小，表示两个分布越接近。

JSD散度：JSD(p||q) = 1/2KL(p||p+q/2) + 1/2KL(q||p+q/2)，就是对称版的KL散度，即JSD(p||q) = JSD(q||p)。

