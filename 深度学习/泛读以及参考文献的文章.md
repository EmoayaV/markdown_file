![image-20221103124704430](D:\markdown file\截图\image-20221103124704430.png)

![image-20221103125058300](D:\markdown file\截图\image-20221103125058300.png)



![image-20221103125214966](D:\markdown file\截图\image-20221103125214966.png)

[TOC]

## **可解释性**

### [【2021】【CVPR】【Yunzhong Hou】【澳大利亚国立大学】【DA中为数不多的将两个域的差异可视化的网络，提出了SFIT网络】Visualizing Adapted Knowledge in Domain Transfer](C:\Users\Emoaya\Desktop\文章\泛读文章\【2021】【CVPR】【Yunzhong Hou】【澳大利亚国立大学】【DA中为数不多的将两个域的差异可视化的网络，提出了SFIT网络】Visualizing Adapted Knowledge in Domain Transfer.pdf)

>[文章解读](https://blog.csdn.net/amusi1994/article/details/116409795)
>
>**干了什么：**
>
>文章基于UDA方法（主要可以用Source-free），提出了一种基于target model和source model下只利用目标域图像生成更加接近源域风格图像的方法（source-free image translation(**SFIT**) approach），解决了在只有模型而无法获取源域数据的情况下（source-free）如何可视化两个域间的知识差异（knowledge difference）的问题，利用模型生成接近源域风格的图像进行可视化的方法，用图像差异代替模型差异，并且利用生成的图像，我们可以进一步对target model进行finetune。
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221031151641916.png" alt="image-20221031151641916" style="zoom:50%;" />
>
>给定源域和目标域网络后，训练一个生成器对目标域图像做变换，使得变换后的图像在源域模型的输出，和原始图像在目标域模型的输出相似。
>
>**创新点：**
>
>DA中为数不多的将两个域的差异可视化的网络，提出了SFIT网络。



## **无监督DA**

## **对抗方法(包括重构方法)**

### [【2022】【CVPR】【Lin Chen】【中科大】【提出DALN网络，提出NWD损失，这个损失可以将分类器作为判别器使用】Reusing the Task-specific Classifier as a Discriminator Discriminator-free Adversarial Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2022】【CVPR】【Lin Chen】【中科大】【提出DALN网络，提出NWD损失，这个损失可以将分类器作为判别器使用】Reusing the Task-specific Classifier as a Discriminator Discriminator-free Adversarial Domain Adaptation.pdf)

>[文章代码](https://github.com/xiaoachen98/DALN)
>
>**干了什么：**
>
>提出了*discriminator-free adversarial learning network (**DALN**)* 网络，引入了*Nuclear-norm Wasserstein discrepancy (**NWD**)* ，NWD可以与分类器耦合作为判别器使用，NWD还可以直接用作通用正则化器，使现有的UDA算法受益。
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221101143518980.png" alt="image-20221101143518980" style="zoom:50%;" />
>
>**创新点：**
>
>提出DALN网络，提出NWD损失，这个损失可以将分类器作为判别器使用。



### [【2022】【ICML】【Harsh Rangwani】【将损失函数进行平滑性约束可以有效的提高模型的泛化性】A Closer Look at Smoothness in Domain Adversarial Training](C:\Users\Emoaya\Desktop\文章\泛读文章\【2022】【ICML】【Harsh Rangwani】【将损失函数进行平滑性约束可以有效的提高模型的泛化性】A Closer Look at Smoothness in Domain Adversarial Training.pdf)

>[文章解读](https://zhuanlan.zhihu.com/p/553511043)
>
>**干了什么：**
>
>由于神经网络运行在过度参数化的模型中，在训练数据损失低并不意味着泛化性能好。**有很多工作显示了在最小值附近的平滑性很大程度上影响了模型的泛化**，本文也是从平滑性入手进行分析。**基于Sharpness Aware Minimization (SAM) 提出一种Smooth Domain Adversarial Training (SDAT) 的训练方式。**
>
>本文的分析从loss的海森矩阵入手，为了衡量海森矩阵的平滑性，本文选择了矩阵的迹Tr(H)以及矩阵最大的特征值λmax作为指标，通过实验上的简单验证，本文发现，越低的λmax会使得DAT训练更加稳定，并且在目标域上效果更好。这里主要的发现即**分类损失越平滑，模型在目标域上的表现更好(没有对域分类器做平滑，不要问为什么，实验证明来的)。**
>
>那么如何才能使得分类器的损失变得更为平滑呢？这也是本文的核心贡献。目前已有工作(SAM)关注于寻找一个局部平滑的最小值点，所以基于SAM的工作，本文对分类器做了平滑性的约束，具体使用的损失函数如下所示。(5)是原始的损失，(11)是经过平滑处理的损失。
>
><img src="D:\markdown file\截图\image-20221101165816175.png" alt="image-20221101165816175" style="zoom: 50%;" />
>
><img src="D:\markdown file\截图\image-20221101165854679.png" alt="image-20221101165854679" style="zoom: 50%;" />
>
>**公式有点多，难懂。**
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221101163328389.png" alt="image-20221101163328389" style="zoom:50%;" />
>
>**创新点：**
>
>将损失函数进行平滑性约束可以有效的提高模型的泛化性。



### [【2021】【NIPS】【Guoqiang Wei】【中科大 微软亚洲研究院】【识别出哪部分是和下游任务相关的，哪些是无关的，然后只对齐和下游任务相关部分的特征】ToAlign Task-oriented Alignment for Unsupervised Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2021】【NIPS】【Guoqiang Wei】【中科大 微软亚洲研究院】【识别出哪部分是和下游任务相关的，哪些是无关的，然后只对齐和下游任务相关部分的特征】ToAlign Task-oriented Alignment for Unsupervised Domain Adaptation.pdf)

>[文章解读](https://zhuanlan.zhihu.com/p/464748555)
>
>**干了什么：**
>
><img src="D:\markdown file\截图\image-20221102142432059.png" alt="image-20221102142432059" style="zoom: 50%;" />
>
><img src="D:\markdown file\截图\image-20221102143340212.png" alt="image-20221102143340212" style="zoom: 50%;" />
>
><img src="D:\markdown file\截图\image-20221102143852415.png" alt="image-20221102143852415" style="zoom:50%;" />
>
>在以往的思路中，通过直接将source domain的表示和target domain表示全部对齐。**然而，有一些和分类任务不相关的表示，这些表示不被对齐对下游任务没有坏处，同时又能让和任务相关部分的表示更好的对齐**。例如，当进行图像中实体分类时，对齐不同domain图像中的实体部分特征表示非常重要，而不同domain图像的背景区域不需要对齐。再例如对不同风格的图片进行分类时，手画的鸟和照相机拍摄的鸟应当都被分类器识别为鸟，所以说图片的风格就是一种与任务不相关的特征，如果把这种风格特征也纳入分类的特征，会影响最后的分类精准度。
>
>**本文的思路为，识别出哪部分是和下游任务相关的，哪些是无关的，然后只对齐和下游任务相关部分的特征**，**我们打算将目标特征与和下游任务相关的源特征对齐，而忽略与任务无关的特征**，这与分类任务的本质是一致的，即关注判别特征，所以我们提出了**T**ask-***o***riented **Align**ment (*ToAlign*) 对齐。
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221102143151391.png" alt="image-20221102143151391" style="zoom:50%;" />
>
>**创新点：**
>
>识别出哪部分是和下游任务相关的，哪些是无关的，然后只对齐和下游任务相关部分的特征，也算是DA中一种类别对齐方法。



### [【2021】【ICCV】【Xiaofeng Liu】【阿里巴巴 卡耐基梅隆大学】【采用了新的假设即存在标签偏移和条件偏移（传统UDA采用协变量偏移假设），提出了一种在训练阶段对条件分布和标签分布进行对齐的方案】Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift Infer, Align and Iterate](C:\Users\Emoaya\Desktop\文章\泛读文章\【2021】【ICCV】【Xiaofeng Liu】【阿里巴巴 卡耐基梅隆大学】【采用了新的假设即存在标签偏移和条件偏移（传统UDA采用协变量偏移假设），提出了一种在训练阶段对条件分布和标签分布进行对齐的方案】Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift Infer, Align and Iterate.pdf)

>[文章解读](https://zhuanlan.zhihu.com/p/464748555)
>
>**干了什么：**
>
><img src="D:\markdown file\截图\image-20221102162830347.png" alt="image-20221102162830347" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221102173310344.png" alt="image-20221102173310344" style="zoom: 33%;" />
>
>背景知识：[关于三个偏移的讲解](https://blog.csdn.net/boke14122621/article/details/104331385)
>
>上图红色部分，即协变量偏移、标签偏移和条件偏移是UDA中常见的。
>
>协变量偏移(covariate shift)：源域与目标与边缘分布不同，条件分布和标签分布均相同。最常见。
>
>标签偏移(label shift)：也叫先验概率偏移，源域和目标域的输出标签 y 的先验分布不同。指源域和目标域每一类样本的数量不一样，比如源域有10只猫样本，2只狗样本，目标域有2只猫样本，10只狗样本。具体可以参考这篇文章[相关链接](./14.【2017】【提出WMMD损失解决了不同域同类别样本分布不均的问题，用到了伪标签】【Hongliang Yan】Mind the class weight bias Weighted maximum mean discrepancy for unsupervised domain adaptation.md)，这篇文章就是争对label shift所作的工作。
>
>条件偏移(conditional shift)：具体到某一类，源域与目标与边缘分布不同。
>
>概念偏移(concept shift)：即输入数据类型分布相同，但是标签发生了变化，比如说这个地方西红柿是蔬菜，那个地方西红柿是水果，一般不太可能有。
>
>由于在目标域中无法访问标签，传统的对抗性UDA方法假设只有协变量偏移，即：假设源域与目标与边缘分布不同，条件分布和标签分布均相同，并依赖于对齐p(x)作为p(x|y)对齐的替代方案。什么意思呢，就是说如果我们本来要对目标域和源域中，每一个类的分布都进行对齐，但是由于目标域没有标签，所以我们就先认为源域和目标域每一类的样本数一致，且每一类的边缘分布都相同，在上面的情况下，我们将目标域和源域样本的整体分布进行对齐，也就把目标域和源域样本的每一类的分布进行了对齐。但是现实中假设只有协变量偏移是不合理的，因为现实中条件分布和标签分布肯定是不相同的，这也是为什么通过对抗过程进行特征对齐后，还要进行类对齐，就是要拉近同类特征的距离，拉开不同类特征的距离。
>
>我们提出了一种内在条件和标签移位下的对抗性无监督域适应(UDA)方法，目的是对齐p(x|y)和p(y)的分布。
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221102175904542.png" alt="image-20221102175904542" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221102175959621.png" alt="image-20221102175959621" style="zoom: 50%;" />
>
>**创新点：**
>
>采用了新的假设即存在标签偏移和条件偏移（传统UDA采用协变量偏移假设），提出了一种在训练阶段对条件分布和标签分布进行对齐的方案。



### [【2021】【ICCV】【Zhiqiang Gao】【西交利物浦大学】【原始的对抗方法将特征输入域判别器，本文的方法将梯度输入域判别器】Gradient Distribution Alignment Certificates Better Adversarial Domain](C:\Users\Emoaya\Desktop\文章\泛读文章\【2021】【ICCV】【Zhiqiang Gao】【西交利物浦大学】【原始的对抗方法将特征输入域判别器，本文的方法将梯度输入域判别器】Gradient Distribution Alignment Certificates Better Adversarial Domain.pdf)

>[文章解读](https://www.baidu.com/link?url=BCVVhAtRTlIG4TzHHkB0hjQOQbmmI0lQOBbnmhgJo_lM_4A8gdl3_EoPiAVu5u2RuGyglNvo25ZBdqF-O-mflgzUffrKvKvVeXOQnlxtr6y&wd=&eqid=c6eabb640005f1fc000000036363331f)
>
>**干了什么：**
>
><img src="D:\markdown file\截图\image-20221103113502635.png" alt="image-20221103113502635" style="zoom:50%;" />
>
>传统的方法大多是基于对抗学习来减小域之间的 discrepancy，但是对抗学习所采用的 MIN-MAX game 并没有一个很好的纳什均衡的保证，即使域分类器完全无法鉴别两个 domain，他们的分布也不一定对齐了，如(b)图。
>
>本文提出了 特征梯度分布对齐方法 feature gradient distribution alignment (**FGDA**)，如(c)图所示，FGDA通过特征提取器和鉴别器之间的对抗学习来**减小特征梯度在两个域之间的分布差异**。当达到平衡时，特征分布差异的值是最小的。
>
>从对抗学习的角度来看，样本的输入梯度可以看作是对输入扰动最小、对模型输出影响最大的敏感方向。直观上，一个样本的特征梯度方向可能倾向于指向其最近决策边界的区域。因此，对齐特征梯度有助于学习潜在表示，从而迫使两个域分布保持更近的距离。这样可以减小特征分布的差异。文章还有一部分理论，验证了 FGDA 可以进一步得到更小的 upper bound。
>**网络：**
>
><img src="D:\markdown file\截图\image-20221103122436457.png" alt="image-20221103122436457" style="zoom:67%;" />
>
>**创新点：**
>
>原本的对抗方法是通过特征提取器和鉴别器之间的对抗学习来**减小特征分布在两个域之间的分布差异**，本文的对抗方法是通过特征提取器和鉴别器之间的对抗学习来**减小特征梯度在两个域之间的分布差异**。简单来说原始的对抗方法将特征输入域判别器，本文的方法将梯度输入域判别器。



### [【2017】【NIPS】【Zelun Luo、Judy Hoffman】【UC伯克利、斯坦福大学】【对于无监督的目标域样本，可以通过计算相似度，转为概率值，最后熵最小化来进行优化】Label Efficient Learning of Transferable](C:\Users\Emoaya\Desktop\文章\泛读文章\【2017】【NIPS】【Zelun Luo、Judy Hoffman】【UC伯克利、斯坦福大学】【对于无监督的目标域样本，可以通过计算相似度，转为概率值，最后熵最小化来进行优化】Label Efficient Learning of Transferable.pdf)

>**干了什么：**
>
>提出了目标域是半监督的情况下，做DA任务的方法，为了加大任务难度，这里目标域带标签的类别可以全是私有类，或者是私有类+共有类。
>
>**网络：**
>
>![image-20221120175057248](D:\markdown file\截图\image-20221120175057248.png)
>
>蓝色的网络是基本的分类学习网络，使用源域数据学习得到， 绿色网络是针对于目标域数据学习网络，初始化为源域网络的参数。
>
>黄色是对抗网络，使用Multi-layer domain adversarial loss。D(.)用来区别数据来源域还是目标域，E()是一个embedding，用来使得S的数据映射到T中，使得D无法区别。网络的下一层不仅使用上一层的数据，还同时使用了S或T相应的E。
>
>损失主要看一下语义转移的损失(semantic transfer loss)，实际上就是先计算相似度，再用softmax转为概率值，最后计算熵，用熵最小化优化。
>
>**创新点：**
>
>语义转移的损失中的思想，就是对于无监督的目标域样本，可以通过计算相似度，转为概率值，最后熵最小化来进行优化。这个思想在这篇文章也有体现：[23.【2020】【NIPS】【Kuniaki Saito】【波士顿大学、MIT】【UDA，DANCE，引入熵分离损失和邻域聚类损失】Universal Domain Adaptation through Self-Supervision](./23.【2020】【NIPS】【Kuniaki Saito】【波士顿大学、MIT】【UDA，DANCE，引入熵分离损失和邻域聚类损失】Universal Domain Adaptation through Self-Supervision.md)



### [【2017】【CVPR】【Eric Tzeng、Judy Hoffman】【UC伯克利、斯坦福大学】【对抗DA方法中loss最好用GAN的，权重最好不要共享】Adversarial Discriminative Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2017】【CVPR】【Eric Tzeng、Judy Hoffman】【UC伯克利、斯坦福大学】【对抗DA方法中loss最好用GAN的，权重最好不要共享】Adversarial Discriminative Domain Adaptation.pdf)

>已经精读。
>
>[25.【2017】【CVPR】【Eric Tzeng、Judy Hoffman】【UC伯克利、斯坦福大学】【对抗DA方法中loss最好用GAN的，权重最好不要共享】Adversarial Discriminative Domain Adaptation](./25.【2017】【CVPR】【Eric Tzeng、Judy Hoffman】【UC伯克利、斯坦福大学】【对抗DA方法中loss最好用GAN的，权重最好不要共享】Adversarial Discriminative Domain Adaptation.md)。



### [【2017】【CVPR】【Konstantinos Bousmalis】【谷歌】【训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容】Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks](C:\Users\Emoaya\Desktop\文章\泛读文章\【2017】【CVPR】【Konstantinos Bousmalis】【谷歌】【训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容】Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks.pdf)

>[文章解读](https://blog.csdn.net/fan1102958151/article/details/106785431)
>
>**干了什么：**
>
>**假设域之间的差异主要是低级别的(由于噪声、分辨率、照明和颜色)，而不是高级别的(对象类型，几何变化等)。**就是说源域只有狗，目标域不能给个猫，图片的颜色等可以变，但是标签不能变。
>
>一般的DA工作都是DA和图像分类同时进行，作者换了一个思路，**训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容。**并提出了一种新的生成对抗网络(GAN)。作者称自己的方法是无监督像素级域自适应方法unsupervised pixel-level domain adaptation(PixelDA)。作者的模型可以用来创建几乎无限的随机样本，这些样本看起来与目标域的图像相似。**这样就可以直接把经过处理的源域图片放入任何训练好的网络进行分类，不需要额外的DA网络来拉近源域和目标域的分布，因为已经在样本上将源域图片分布变为目标域图片的分布。**
>
>作者的模型将域适应过程与任务特定分类过程解耦，因为它的主要功能是适应源域的图像，使它们看起来就像从目标域采样一样。**简单说这个网络就只负责DA任务，训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容。**
>
>**网络：**
>
>![image-20221106114941778](D:\markdown file\截图\image-20221106114941778.png)
>
>在训练时，先把源域的照片xs和噪声z经过生成器G变为假图片xf，再把假图片xf和真图片(目标域图片)xt放入判别器D，判别器D试图区分由生成器产生的假图像Xf和来自目标域Xt的“真实”图像，除此之外，还加入了一个分类器T，将假照片xf和源域照片xs进行分类。
>
>**损失函数：**
>
>总损失：
>
><img src="D:\markdown file\截图\image-20221106123123989.png" alt="image-20221106123123989" style="zoom:50%;" />
>
>域损失，主要用对抗过程拉近两个域的分布。域损失，主要用对抗过程拉近两个域的分布：
>
><img src="D:\markdown file\截图\image-20221106123234446.png" alt="image-20221106123234446" style="zoom:50%;" />
>
>分类损失，将源域图片和生成图片进行分类：
>
><img src="D:\markdown file\截图\image-20221106123213071.png" alt="image-20221106123213071" style="zoom:50%;" />
>
>PMSE损失，这种损失使模型能够学习重现要建模的对象的整体形状，而不会在输入的绝对颜色或强度上浪费建模能力，同时又使我们的对抗训练得以以一致的方式更改对象。 请注意，损失并不会阻止前景改变，而是会鼓励前景以一致的方式发生变化。 在这项工作中，由于数据的性质，文中对单个前景对象应用了蒙版的PMSE损失，但是可以将其微不足道地扩展到多个前景对象。主要用于重构：
>
><img src="D:\markdown file\截图\image-20221106123548667.png" alt="image-20221106123548667" style="zoom:50%;" />
>
>**创新点：**
>
>1.训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容。
>
>2.将域适应过程与任务特定分类过程解耦，简单说这个网络就只负责DA任务，训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容。



### [【2016】【NIPS】【Konstantinos Bousmalis】【谷歌】【利用重构的方法提取不同域之间的公有特征】Domain Separation Networks](C:\Users\Emoaya\Desktop\文章\泛读文章\【2016】【NIPS】【Konstantinos Bousmalis】【谷歌】【利用重构的方法提取不同域之间的公有特征】Domain Separation Networks.pdf)

>[文章解读](https://zhuanlan.zhihu.com/p/49479734)
>
>**干了什么：**
>
>提出一个新网络，**Domain Separation Networks (DSN)**，我们将分布中的“低水平”差异定义为由噪声、分辨率、光照和颜色引起的差异。“高级”差异与类的数量、对象的类型和几何变化有关，我们**假设源域和目标域的差异主要在于低水平图像统计量的分布**，而它们的高水平参数分布相似，标签空间相同，与高水平参数无关。他们认为所有的域之间有着公有的特征(Shared)和私有的特征(Private)，如果将各个域的私有特征也进行迁移的话就会造成负迁移(negative transfer)。
>
>从上面可以看出，DSNs的主要工作分为两部分：(1)提取不同域之间的公有特征 (2)利用公有特征进行迁移。
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221105141101531.png" alt="image-20221105141101531" style="zoom:50%;" />
>
>DSNs的主体结构是一个类似于自编码器的结构。整个结构可以分为如下五部分：
>
>- Private Target Encoder Ept(xt) : 目标域私有编码器，用来提取目标域的私有特征。
>- Private Source Encoder Eps(xs) : 源域私有编码器，用来提取源域的私有特征。
>- Shared Encoder Ec(x) : 共享编码器，用来提取源域和目标域的公有特征。
>- Shared Decoder D(Ec(x)+Ep(x)) : 共享解码器，用来将私有特征和公有特征组成的样本进行解码。
>- Classifier G(Ec(xs)) : 分类器，在训练时用来对源域样本进行分类，在训练完成时就可以直接用在目标域数据上进行分类。
>
>**训练流程和损失：**
>
>看过每个部分的功能之后其实整个结构的原理就很明了了，首先我们忽略右下角的分类器，那么剩下的部分就是自编码器的结构。源域样本 xs首先进入Eps(xs) 和Ec(x)，之后两个编码器分别输出 hps 和 hcs 。hps 和 hcs分别对应着源域数据中的私有特征和公有特征，同理，hpt 和 hct分别对应着目标域数据中的私有特征和公有特征。从上述的原理介绍我们可以知道，如果DSNs想要取得较好的效果，那么公有部分必须是真实的源域和目标域共有的特征，同时公有部分和私有部分必须完全分开，才能有效的避免负迁移，所以作者提出了以下两种损失函数来约束Ept(xt)，Eps(xs)和Ec(x)，首先是‘差异损失’：
>
><img src="D:\markdown file\截图\image-20221105181318253.png" alt="image-20221105181318253" style="zoom:50%;" />
>
>Ldifference其实计算的是hps 和 hcs以及hpt 和 hct的相似度大小，当hps = hcs以及hpt = hct时Ldifference最大，当hps 和 hcs正交（完全不同）以及hpt 和 hct正交时Ldifference最小。所以通过最小化Ldifference可以达到 hps 和 hcs 以及hpt 和 hct完全分开的目的。
>
>完全分开hps 和 hcs 以及hpt 和 hct是不够的，因为我们还要保证 hcs和 hct 是可以进行迁移的，也就是要将两者进行适配，直观体现就是提高两者的相似度，因此作者提出了两种‘相似损失’：
>
><img src="D:\markdown file\截图\image-20221105181851666.png" alt="image-20221105181851666" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221105181911963.png" alt="image-20221105181911963" style="zoom:50%;" />
>
>这两种损失都衡量了hcs和 hct差异的大小，因此当Lsimilarity最小时可以使hcs和 hct最相似甚至变为同一种分布。当hcs和 hct的分布近似相等时，在 hcs 上有效的分类器同样也可以在 hct 上工作了。其中DANN的损失较好。
>
>这种‘编码-解码’这种结构不仅能够保证提取公有、私有特征，还能够保证特征的完整性和有效性。具体的，作者为Shared Decoder提出了‘重构损失’：
>
><img src="D:\markdown file\截图\image-20221105194236752.png" alt="image-20221105194236752" style="zoom:50%;" />
>
>对于源域样本而言，Shared Decoder D(Ec(x)+Ep(x))的输入是 hps+hcs ，输出是解码得到的 x¯s ,并且在训练时需要最小化重构损失。到此，源域样本进入编码器到从解码器出来的过程中，两个解码器分别提取了私有和公有特征，同时又因为解码器要求公有特征和私有特征组合在一起要能够构成完整的源域样本，所以又保证了特征的完整性。同理，当目标域样本进入时也是同样的操作。
>
>**但是上面只是直觉上的理解，毕竟如果decoder比较好，连噪声都可以还原为源样本。**
>
>到目前为止，我们通过‘编码-解码’这种无监督方法提取出来了源域和目标域的公有特征以及各自的私有特征，但是我们还无法对样本进行分类，所以作者在整个结构中加入了分类器 G(Ec(xs)) ，其损失函数为：
>
><img src="D:\markdown file\截图\image-20221105194520402.png" alt="image-20221105194520402" style="zoom:50%;" />
>
>总损失为：
>
><img src="D:\markdown file\截图\image-20221105194555373.png" alt="image-20221105194555373" style="zoom:50%;" />
>
>**创新点：**
>
>本文的思想和[【2021】【NIPS】【Guoqiang Wei】【中科大 微软亚洲研究院】【识别出哪部分是和下游任务相关的，哪些是无关的，然后只对齐和下游任务相关部分的特征】ToAlign Task-oriented Alignment for Unsupervised Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2021】【NIPS】【Guoqiang Wei】【中科大 微软亚洲研究院】【识别出哪部分是和下游任务相关的，哪些是无关的，然后只对齐和下游任务相关部分的特征】ToAlign Task-oriented Alignment for Unsupervised Domain Adaptation.pdf)这篇文章差不多，意思都是(1)**提取不同域之间的公有特征** (2)利用公有特征来训练网络，使其学习域不变的特征，再放入分类网络进行分类。
>
>利用重构的方法。



### [【2016】【ECCV】【Muhammad Ghifary】【惠灵顿维多利亚大学】【DRCN，用重构的方法但是不严谨】Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2016】【ECCV】【Muhammad Ghifary】【惠灵顿维多利亚大学】【DRCN，用重构的方法但是不严谨】Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation.pdf)

>**干了什么：**
>
>用重构的方法来做DA，提出了深度重构分类网络 Deep Reconstruction Classifification Network (**DRCN**) 的新模型。
>
>**目标域重构器**是用来降低目标域重构损失，即使得提取的特征能够经过重构器之后尽可能重现目标域样本，**如果Decoder最终的输出和原始Encoder的输入差距很小，我们就可以认为Encoder部分学习到的关于目标域的特征表达是有效的。但是这里有个疑问， 即便你可以很准确的提取目标域特征，那然后呢？对于域分类有什么帮助呢？分类器的参数可没根据目标域特征来进行调整，分类器的参数是用来最大提高源域数据分类精准度的，你目标域提取到的特征再准确，它未必能够被分类器来正向利用，只有提取域不变特征才能被分类器利用。**
>
>看下面的公式(4)，重构的损失是将目标域样本经过encoder和decoder后，将输出与目标域样本做损失，感觉没什么意义。
>
>**损失和算法：**
>
><img src="D:\markdown file\截图\image-20221105133626470.png" alt="image-20221105133626470" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221105133654643.png" alt="image-20221105133654643" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221105133726062.png" alt="image-20221105133726062" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221105133741915.png" alt="image-20221105133741915" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221105133809642.png" alt="image-20221105133809642" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221105133841244.png" alt="image-20221105133841244" style="zoom:50%;" />
>
>**网络：**
>
><img src="D:\markdown file\截图\image-20221105132718996.png" alt="image-20221105132718996" style="zoom:50%;" />
>
>**创新点和问题：**
>
>用重构的方法来做DA，提出了深度重构分类网络。想法不错但是有很大的漏洞，逻辑上说不通。
>
>问题1：用编解码器来重构目标域的样本，怎么确保这样提出来的特征就是域不变的？特征提取器网络完全可以光根据源域的样本来提特征，放入分类器完成分类任务，而对目标域样本完全不管，通过后面的decoder网络，也可以使得目标域样本达到很好的重构。比如GAN网络，别说输入一组特征，就算输入一组噪声，经过合理的训练都可以输出图片。
>
>问题2：就算提取出了域不变的特征，那么分类器只在源域的样本上训练过，怎么确保加入目标域的样本还能准确分类？



### [【2016】【JMLR】【Yaroslav Ganin】【将他2015年文章提出的网络进行命名，Domain-Adversarial Neural Networks (DANN)】Domain-Adversarial Training of Neural Networks](C:\Users\Emoaya\Desktop\文章\泛读文章\【2016】【JMLR】【Yaroslav Ganin】【将他2015年文章提出的网络进行命名，Domain-Adversarial Neural Networks (DANN)】Domain-Adversarial Training of Neural Networks.pdf)

>**干了什么：**
>
>就是将他2015年文章提出的网络进行命名，Domain-Adversarial Neural Networks (**DANN**)。
>
>**网络：**
>
>![image-20221103140501073](D:\markdown file\截图\image-20221103140501073.png)
>
>**创新点：**
>
>无。



### [【2015】【ICML】【Yaroslav Ganin】【提出用对抗的方式做DA】Unsupervised Domain Adaptation by Backpropagation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2015】【ICML】【Yaroslav Ganin】【提出用对抗的方式做DA】Unsupervised Domain Adaptation by Backpropagation.pdf)

>已经精读。[链接](./9.【2015】【DANN，将对抗性训练的思想使用到域适应中，梯度反转层】【Yaroslav Ganin】Unsupervised Domain Adaptation by Backpropagation.md)



## **参考文献文章**

### [【2014】【NIPS】【David Eigen】【纽约大学】【尺度不变误差 scale-invariant error】Depth Map Prediction from a Single Image using a Multi-Scale Deep Network](C:\Users\Emoaya\Desktop\文章\泛读文章\【2014】【NIPS】【David Eigen】【纽约大学】【尺度不变误差 scale-invariant error】Depth Map Prediction from a Single Image using a Multi-Scale Deep Network.pdf)

>**干了什么：**
>
>任务是单目深度估计：即用一个图片估计深度，实际上这个任务几乎和语义分割差不多。
>
>本篇文章不是DA的文章，文章首次利用CNN估计单目图像深度的论文，主要分为两部分：1）全局深度估计；2）局部特征深度精估计，同时在损失计算上提出了**尺度不变误差scale-invariant error**。
>
>预测深度有助于提供更丰富的目标及其环境表示，通常会改进现有的识别任务。
>
>**尺度不变误差*scale-invariant error***解决有比例缩放的情况，比如一个普通的房间对一个玩偶屋。
>
>**网络结构：**
>
><img src="D:\markdown file\截图\image-20221108155824428.png" alt="image-20221108155824428" style="zoom:50%;" />
>
>实际上这个网络其实可以理解为图像分割网络的变种。
>
>该网络分为两个模块，一个是“粗糙尺度网络”，那么这个网络模块首先对场景的全局深度进行了一个大概的预测，得到了哪里是远，哪里是近的一个预测。另一个是“精细尺度网络”，在场景粗略深度的基础上，该模块对场景内的局部区域进行精细化的深度预测。
>
>全局粗糙尺度网络(上半部分)：这一部分的任务是利用场景的全局视图预测全局深度图结构，提取出全局的信息，比如“消失点”，“物体的位置”，还有“房间的大致框架”。感觉就像是近视的人摘了眼镜看到的场景。
>
>局部精细尺度网络(下半部分)：这一部分的任务是对粗糙预测结果进行修改，让其与物体，墙的边缘等局部信息进行对齐。
>
>**尺度不变误差：这是重点，可以说整篇文章啥都可以不看，看这个就行。**
>
>关于尺度不变误差的解读：
>
>[文章解读1](https://blog.csdn.net/MTandHJ/article/details/113717522)
>
>[文章解读2](https://blog.csdn.net/chishuideyu/article/details/83573174)
>
>尺度不变RMSE与其他RMSE的对比：
>
><img src="D:\markdown file\截图\image-20221108172315449.png" alt="image-20221108172315449" style="zoom:33%;" />
>
><img src="D:\markdown file\截图\image-20221108172643322.png" alt="image-20221108172643322" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221108172602146.png" alt="image-20221108172602146" style="zoom:50%;" />
>
>~~看公式(1)，假如没有后面的α项，且y-y^*^=0，那么当预测的像素y都加1时，y-y^*^=1会产生较大的损失，我们希望在上面那种情况下，损失依然很小，那么就在后面加一项α，α是这张图片的预测像素值与真实值的差值的平均。回到上面的例子，若y-y^*^=1，那么α=1/nΣy^*^-y=-1，y-y^*^+α=0。~~
>
>~~还可以这样理解，把y-y^*^看作为一个值，那么α=1/nΣy^*^-y就是这个值的均值，也就是说我们减小他的方差，而不是具体的值。~~
>
>实在不行就理解为升级版的MSE损失。
>
>**创新点：**
>
>尺度不变误差 scale-invariant error。



### [【2019】【CVPR】【Woong-Gi Chang】【韩国首尔国立大学】【争对于DA任务，将BN层换位DSBN层】Domain-Specific Batch Normalization for Unsupervised Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2019】【CVPR】【Woong-Gi Chang】【韩国首尔国立大学】【争对于DA任务，将BN层换位DSBN层】Domain-Specific Batch Normalization for Unsupervised Domain Adaptation.pdf)

>**干了什么：**
>
>原始的DA任务中源域和目标域共用一个网络包括BN层，这明显不合理，作者争对于BN做了改进。
>
>**我们提出了Domain-Specific Batch Normalization (DSBN)基于域的批量归一化**。DSBN使用BN参数捕获基于域的信息，并使用参数将基于域的特征转换为域不变的特征。DSBN和BN一样，易于扩展。
>
>**我们还提出了一种新的基于DSBN的无监督领域自适应框架**。
>
>**DSBN结构：**
>
><img src="D:\markdown file\截图\image-20221121151707416.png" alt="image-20221121151707416" style="zoom: 50%;" />
>
>想法很简单，就是在DA任务中，一个批量有两个域的样本，源域的样本用一个BN，目标域样本用一个BN，并且这两个BN的参数不共享。
>
>**网络结构：**
>
><img src="D:\markdown file\截图\image-20221121152520648.png" alt="image-20221121152520648" style="zoom:50%;" />
>
>这个网络是两阶段的训练，首先给目标域样本生成伪标签(F1网络，其中的BN层全部换为DSBN层)，然后将源域的样本和带伪标签的目标域样本一块放入网络，进行训练(F2网络，其中的BN层全部换为DSBN层)。
>
>**创新点：**
>
>实际上这篇文章创新点在于DSBN，网络没什么创新点。



### [【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy](C:\Users\Emoaya\Desktop\文章\泛读文章\【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy.pdf)

>已经精读。
>
>[33.【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy](./33.【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy.md)



### [【2018】【CVPR】【Zhirong Wu】【UC伯克利、香港中文大学】【引入一种新的无监督分类任务的方法】Unsupervised Feature Learning via Non-Parametric Instance Discrimination](C:\Users\Emoaya\Desktop\文章\泛读文章\【2018】【CVPR】【Zhirong Wu】【UC伯克利、香港中文大学】【引入一种新的无监督分类任务的方法】Unsupervised Feature Learning via Non-Parametric Instance Discrimination.pdf)

>已经精读。
>
>[34.【2018】【CVPR】【Zhirong Wu】【UC伯克利、香港中文大学】【引入一种新的无监督分类任务的方法】Unsupervised Feature Learning via Non-Parametric Instance Discrimination](./34.【2018】【CVPR】【Zhirong Wu】【UC伯克利、香港中文大学】【引入一种新的无监督分类任务的方法】Unsupervised Feature Learning via Non-Parametric Instance Discrimination.md)



### [【2017】【ICML】【Mingsheng Long】【清华大学】【提出JAN网络和JMMD损失】Deep Transfer Learning with Joint Adaptation Networks](C:\Users\Emoaya\Desktop\文章\泛读文章\【2017】【ICML】【Mingsheng Long】【清华大学】【提出JAN网络和JMMD损失】Deep Transfer Learning with Joint Adaptation Networks.pdf)

>**干了什么：**
>
>JAN提出JMMD的方法，目的是度量网络最后几层的联合分布的差异。前面的DDC或者是DAN等都只考虑隐藏层源域和目标域单层之间的距离，但是有时候度量联合分布之间的差异效果更好。
>
>**网络结构：**
>
>JAN2017：
>
><img src="D:\markdown file\截图\image-20221109143543613.png" alt="image-20221109143543613" style="zoom:50%;" />
>
>RTN2016：
>
><img src="D:\markdown file\截图\image-20221109143930237.png" alt="image-20221109143930237" style="zoom:50%;" />
>
>DAN2015：
>
><img src="D:\markdown file\截图\image-20221109144149820.png" alt="image-20221109144149820" style="zoom:50%;" />
>
>DDC2014：
>
><img src="D:\markdown file\截图\image-20221109150209704.png" alt="image-20221109150209704" style="zoom:50%;" />
>
>**损失函数：**
>
>损失函数和前面的DAN网络一致，只不过MMD换成了JMMD：
>
><img src="D:\markdown file\截图\image-20221109143154004.png" alt="image-20221109143154004" style="zoom:50%;" />
>
>JMMD为：
>
><img src="D:\markdown file\截图\image-20221109143352214.png" alt="image-20221109143352214" style="zoom:50%;" />
>
><img src="D:\markdown file\截图\image-20221109143406849.png" alt="image-20221109143406849" style="zoom:50%;" />
>
>**创新点：**
>
>没有什么突破性的创新，无论是DAN、RTN还是JAN，就是在DDC网络基础上不断换损失。



### [【2016】【AAAI】【Baochen Sun】【UC伯克利】【对Coral进行了复述，无创新点】Return of Frustratingly Easy Domain Adaptation](C:\Users\Emoaya\Desktop\文章\泛读文章\【2016】【AAAI】【Baochen Sun】【UC伯克利】【对Coral进行了复述，无创新点】Return of Frustratingly Easy Domain Adaptation.pdf)

>略。基本与[24.【2016】【ECCV】【Baochen Sun】【波士顿大学】【引入Coral损失，比MMD简单效果还好】Deep CORAL Correlation Alignment for Deep Domain Adaptation](./24.【2016】【ECCV】【Baochen Sun】【波士顿大学】【引入Coral损失，比MMD简单效果还好】Deep CORAL Correlation Alignment for Deep Domain Adaptation.md)一致。只不过本文着重于推导。



### [【2018】【ECCV】【Yang Zou】【卡耐基梅隆大学】【使用伪标签在图像分割任务上进行域适应】Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training](C:\Users\Emoaya\Desktop\文章\泛读文章\【2018】【ECCV】【Yang Zou】【卡耐基梅隆大学】【使用伪标签在图像分割任务上进行域适应】Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training.pdf)

>[文章解读](https://blog.csdn.net/fuyouzhiyi/article/details/83780221)
>
>**干了什么：**
>
>就是伪标签的常规用法。
>
>**网络结构：**
>
><img src="D:\markdown file\截图\image-20221120173556410.png" alt="image-20221120173556410" style="zoom:67%;" />
>
>**创新点：**
>
>没什么创新点。
