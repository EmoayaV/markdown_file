## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\17.【2015】【ICML】【Mingsheng Long】【清华大学、UC伯克利】【DAN，是DDC的进化版，将MMD损失用在了倒数第1、2、3层】Learning Transferable Features with Deep Adaptation Networks.pdf)

### 1.1 在干什么、有什么贡献、什么结论

**背景：目标域无监督。**

精简的说，研究表明：深度[神经网络](https://so.csdn.net/so/search?q=神经网络&spm=1001.2101.3001.7020)可以学习可迁移特征，这些特征用于域适应时在新的任务上表现出很好的泛化能力。然而由于深度特征随着网络层数的增加由一般到特殊转变，特征的可迁移能力在网络高层急剧下降，极大地增加了域之间的差异性。

神经网络通常在前面几层都学习到的是通用的特征（general feature），随着网络的加深，后面的网络更偏重于学习特定的特征（specific feature）。

其中有两篇经典论文可以作为背景来介绍。

* [3.【2014】【NIPS】【Yosinski、Bengio】【康奈尔大学】【在不同的层数进行finetune和frozen看效果】How transferable are features in deep neural networks](./3.【2014】【NIPS】【Yosinski、Bengio】【康奈尔大学】【在不同的层数进行finetune和frozen看效果】How transferable are features in deep neural networks.md)

* [16.【2014】【arXiv】【Eric Tzeng、Hoffman】【UC伯克利】【DDC，将MMD用在了倒数第二层】Deep Domain Confusion Maximizing for Domain Invariance](./16.【2014】【arXiv】【Eric Tzeng、Hoffman】【UC伯克利】【DDC，将MMD用在了倒数第二层】Deep Domain Confusion Maximizing for Domain Invariance.md)

## 2. 网络结构

**DAN是在DDC的基础上发展起来的，它很好地解决了DDC的两个问题：**

 1.DDC只适配了一层网络，可能还是不够，因为之前的工作中已经明确指出不同层都是可以迁移的。所以DAN就多适配几层。

 2.DDC是用了单一核的MMD，单一固定的核可能不是最优的核。DAN用了多核的MMD（MK-MMD），效果比DDC更好。

DAN主要思想：通过明确地减少域差异来增强深度学习神经网络的具体任务层的特征迁移性。（域差异可以通过使用平均嵌入匹配的最佳多核选择方法来被进一步减小。）

 为了实现这一目标，所有具体任务层的隐藏表示被嵌入到Reproducing kernel Hilbert space，在希尔伯特空间中不同的域分布的平均嵌入可以被明确分配。由于平均嵌入对内核的选择非常敏感，所以设计最优多内核选择过程，这可以进一步减少域差异。

**创新点一：多层适配**

 DAN也基于AlexNet网络，适配最后三层（6~8层）。为什么是这三层？在前面的图中可以看出，网络的迁移能力在这三层开始就会特别地task-specific，所以要着重适配这三层。至于别的网络（比如GoogLeNet、VGG）等是不是这三层那就不知道了，那得一层一层地计算相似度。DAN只关注使用AlexNet。

结合自己的实验时，需要考虑适配哪几层。

**创新点二：MK-MMD(Multi-kernel MMD)**

传统学习方法有一个假设：training sample和test sample都是从同一个分布抽样得到，即训练集和测试集是独立同分布的，这个假设使得离线的学习方法得以运行。在迁移学习环境下training sample和test sample分别取样自分布p和q，两个样本不同但相关，现在我们要通过测试样本改善模型性能。求解这个问题的思路比较多，这里只列一个目前流行的思路：

 利用深度神经网络的特征变换能力，来做特征空间的transformation，直到变换后的特征分布相匹配，这个过程可以是source domain一直变换直到匹配target domain，也可以是source domain和target domain一起变换直到匹配（例如下图）

<img src="D:\markdown file\截图\image-20221015202146369.png" alt="image-20221015202146369" style="zoom:50%;" />

**对该图的理解：**源域和目标域的数据都放在一起，通过AlexNet来训练，前三层frozen，第四层第五层fine-tuning，当到后面几层时，source data和target data分开，然后通过MK-MMD方法来计算两个域的距离，并且通过损失函数来进行优化，最后当损失函数优化到设定的阈值时，就可进行最终的分类

概率分布p和q之间的MK-MMD dk（p，q）被定义为p和q的平均嵌入之间的RKHS距离。对于两个概率分布，它们的MK-MMD距离平方就是：
![image-20221015193821508](D:\markdown file\截图\image-20221015193821508.png)

**这个多个核一起定义的kernel就是：**

![image-20221015193841316](D:\markdown file\截图\image-20221015193841316.png)

**总的方法(DAN)：**

它的优化目标由两部分组成：损失函数和分布距离。损失函数用来度量预测值和真实值的差异。分布距离就是我们上面提到的MK-MMD距离。于是，DAN的优化目标就是：

![image-20221015193946332](D:\markdown file\截图\image-20221015193946332.png)

其中，λ可以控制MMD的限制程度。如下图，该损失函数不仅能使模型对训练集进行很好地分类，并能让训练集和测试集的分布尽可能地相近。

**训练过程：**

1.先把网络用源域的样本训练好，或者预训练好。

2.再把源域的数据和目标域的数据分别放入网络提取特征，在网络的第6、7、8层放3个MMD损失，因为源域的数据是有标签的，所以在源域数据的输出加一个交叉熵损失，目标域的数据输出后则不用加交叉熵损失。