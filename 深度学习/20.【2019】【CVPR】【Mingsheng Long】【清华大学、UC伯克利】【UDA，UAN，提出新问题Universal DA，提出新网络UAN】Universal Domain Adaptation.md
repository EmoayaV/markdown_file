## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\20.【2019】【CVPR】【Mingsheng Long】【清华大学、UC伯克利】【UDA，UAN，提出新问题Universal DA，提出新网络UAN】Universal Domain Adaptation.pdf)

### 1.1 在干什么、有什么贡献、什么结论

**背景：目标域无监督。**

**提出了新模型：*Universal Adaptation Network (UAN)*。**

这篇论文主要是提出了一种新的问题设置，Domain gap和category gap并存的情况。**domain gap** 是指源域和目标域数据特征存在差异，**category gap **是指源域和目标域的标签空间存在差异。

![image-20221023192644240](D:\markdown file\截图\image-20221023192644240.png)

传统的域自适应是源域和目标域之间只有特征不同，但是标签空间是相同的，**这篇论文总结出源域和目标域之间的标签空间存在closed,partial,open set三种关系（如上图所示）**。这三种关系之前的工作中也有人做过，只是**这篇文章对这三种情况加以总结，并提出一个通用的方法，能够解决三种关系中的任意一种。所以说在这之后，所有的closed set DA,partial DA,open set DA问题统称为Universal Domain Adaptation (UDA)。**

<img src="D:\markdown file\截图\image-20221023185845882.png" alt="image-20221023185845882" style="zoom:50%;" />

现有的域适应方法是不适应这种UDA问题设置的，可能会导致源域独有的样本和目标域独有的样本进行特征对齐，如上图中左下角的子图，特征对齐方法可能会强行把熊和鸟对齐。

如何解决这种问题呢？这类问题的**核心是如何找到common label set**，也就是源域和目标域标签集合相交的部分。如果目标域样本的标签在公共集合中，则对其分类；如果不在，也就是说是目标域私有的标签，则标记为unknown。这个核心思路非常重要，由这篇论文衍生出的几十篇论文都是这个思路。

## 2. 符号表示

![image-20221023194934432](D:\markdown file\截图\image-20221023194934432.png)

## 3. 面临的挑战

(1)如果对UDA的问题使用传统DA的方法（也就是closed set DA的方法，传统DA方法只考虑domain gap，而没考虑category gap）将特征强行对齐，那么可能将目标域有而源域没有的类，强行分类成源域已有的类。

(2)如果使用 partial DA和open set DA方法呢？因为我们对目标域的标签集（就是不知道目标域有几类）一无所知，所以我们没办法选DA的方法，所以使用 partial DA和open set DA方法这种情况也是不可行。

(3)UDA还是存在domain gap 的，如何将公共标签集C（就是源域和目标域的公共类）中的源数据和目标数据的分布进行对齐也是个问题。

(4)如何去检测出目标域的未知类别也是个问题，其实这个问题也可以转换为如何找到公共类别，如果找到公共类别，那么其他类别自然就是未知类别。

## 4. 网络结构

![image-20221023210534778](D:\markdown file\截图\image-20221023210534778.png)

**UAN网络结构有四部分：特征提取器F，对抗的域判别器D，非对抗的域判别器D'，标签分类器G。**

特征提取器F：提取特征。

标签分类器G：用源域的带标签的数据训练分类器。

对抗的域判别器D：将源域和目标域的公共类进行特征对齐，拉近源域和目标域公共类别的分布。

非对抗的域判别器D'：获得目标域和源域的相似度。

**损失函数：**

<img src="D:\markdown file\截图\image-20221024122140330.png" alt="image-20221024122140330" style="zoom:50%;" />

前提条件：源域标签1，目标域标签0。

公式(1)表示在源域上的分类损失，用来使网络能够正确分类。

公式(2)表示域判别器的损失（因为这个域判别器不参与对抗过程，所以也叫做非对抗-域判别器），主要的作用是得到域相似度（domain similarity）。

公式(3)表示域判别器损失（这个域判别器参与对抗过程，所以也叫做对抗-域判别器），其中**w^s^(x)表示源域数据属于公共标签集（common label set）的概率，w^t^(x)表示目标域数据属于公共标签集（common label set）的概率，**主要目的是为了在公共标签集下，对齐源域和目标域的特征。

**优化：**

<img src="D:\markdown file\截图\image-20221024140926969.png" alt="image-20221024140926969" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221024122513245.png" alt="image-20221024122513245" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221024122440749.png" alt="image-20221024122440749" style="zoom:50%;" />

公式(4)是损失函数的更新过程，其中min~G~ E~G~是更新标签分类器的过程，略。min~d‘~ E~D’~是更新非对抗-域判别器的过程，其实也是更新一个二分类器，略。

重点看max~D~ min~F~ -λE~D~的更新过程，其实这个更新过程与GAN中，公式(1)的 min~G~ max~D~ V~D,G~的更新过程是一样的，甚至损失函数都差不多，所以更新过程可以看看GAN是怎么更新的，方便理解，不过作者下面也说了**使用GRL梯度反转层更新**，不过也差不多。

**预测阶段：**

<img src="D:\markdown file\截图\image-20221024143755964.png" alt="image-20221024143755964" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221024144237772.png" alt="image-20221024144237772" style="zoom:50%;" />

(1)输入样本x，得到特征z。

(2)特征z输入到标签分类器G（G是在源域包含的类别上训练的）得到分类概率 y^^^(x)。

(3)同时把特征z输入到非对抗-域分类器D‘，得到域相似度 d^^^(x)。

(4)计算目标域数据属于公共标签集（common label set）的概率w^t^(x)。

(5)设置阈值w~0~，使用公式(5)进行预测。

**如何计算w^t^(x)和w^s^(x)：**

<img src="D:\markdown file\截图\image-20221024144915492.png" alt="image-20221024144915492" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221024144955960.png" alt="image-20221024144955960" style="zoom: 50%;" />

<img src="D:\markdown file\截图\image-20221024145116956.png" alt="image-20221024145116956" style="zoom:50%;" />

**前提条件：源域标签1，目标域标签0。**

我们输入一个样本x，可以得到y^^^，d^^^，d^^^’，但是D是对抗-域分类器，参与对抗过程，所以其结果d^^^可能导致判别不准，所以作者这里引入一个非对抗-域分类器，其结果d^^^‘专门对源域和目标域进行分类，所以在计算w^t^(x)和w^s^(x)时，我们只使用d^^^’和y^^^。

首先，我们认为w^t^(x)和w^s^(x)满足**公式(6)的假设**。意思很简单。比如第一个式子，就是说**源域样本的标签如果属于公共标签集，那么它是公共标签集的概率就大于源域独有的标签的样本的概率（现实不一定，但做实验可以人为满足）。**在这个假设下，可以推出第一个粉线和第二个粉线的假设。

d^^^‘可以看作每个样本的域相似度（domain similarity）的量化，那么**对于源域样本来说，d^^^’越小意味着与目标域越接近，对于目标域样本来说，d^^^‘越大意味着与源域越接近**。那么就可以推出**第一个粉线所做的假设**，即：源域多出类别的d^^^'>源域公共类别的d^^^'>目标域公共类别的d^^^'>目标域多出类别的d^^^'。

直觉上其实也好理解，源域多出类别的d^^^'是最大的，因为目标域没有和他相同的类别去混淆他，所以判别器应该很准确的进行判别，可能为0.9。源域公共类别的d^^^'第二大，因为目标域有和他相同的类别去混淆他，但是都是由源域的样本进行计算，所以肯定比0.5大。目标域公共类别的d^^^'第三大，因为源域有和他相同的类别去混淆他，但是都是由目标域域的样本进行计算，所以肯定比0.5小。目标域域多出类别的d^^^'是最小的，因为源域没有和他相同的类别去混淆他，所以判别器应该很准确的进行判别，可能为0.1。

y^^^是每个类别的概率值，熵可以量化预测的不确定性，更小的熵意味着更确定的预测，**H(y^^^) 如果样本来自源域，则H(y^^^)越大，说明样本越可能属于公共类，如果样本来自目标域，则H(y^^^)越小，说明样本越可能属于公共类。**那么就可以推出**第二个粉线所做的假设**，即：目标域多出类别的H(y^^^)>目标域公共类别的H(y^^^)>源域公共类别的H(y^^^)>源域多出类别的H(y^^^)。

**注意上面的都是假设，没有数学推导。**

<img src="D:\markdown file\截图\image-20221024145148569.png" alt="image-20221024145148569" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221024160712029.png" alt="image-20221024160712029" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221024161032241.png" alt="image-20221024161032241" style="zoom: 33%;" />

**首先，明确w^s^(x)表示源域数据属于公共标签集（common label set）的概率，w^t^(x)表示目标域数据属于公共标签集（common label set）的概率。**

那么根据上面的假设，先对熵进行归一化，log|C~s~|表示源域的最大熵。①为什么用源域的最大熵做归一化？因为我们预测的类别只基于源域，超出源域的类别判别为未知。②为什么最大熵是log|C~s~|？假设4类，则最大熵为log 4，那么C~s~类最大熵自然是log|C~s~|。归一化后可以看到H(y^^^)/log|C~s~|值域[0, 1]，d^^^’值域[0, 1]。

接下来我们就可以计算w^t^(x)和w^s^(x)，先写出上面的两个假设：

*对于源域样本来说，d^^^’越小意味着与目标域越接近，对于目标域样本来说，d^^^‘越大意味着与源域越接近，即：源域多出类别的d^^^'>源域公共类别的d^^^'>目标域公共类别的d^^^'>目标域多出类别的d^^^'*

*H(y^^^) 如果样本来自源域，则H(y^^^)越大，说明样本越可能属于公共类，如果样本来自目标域，则H(y^^^)越小，说明样本越可能属公共类，即：目标域多出类别的H(y^^^)>目标域公共类别的H(y^^^)>源域公共类别的H(y^^^)>源域多出类别的H(y^^^)*

**有2个情况，一一分析。**

情况一：x是源域多出类别的样本和x是源域公共类别的样本，那么：

E~x属于源域私有类~H(y^^^) -  E~x属于源域私有类~d^^^’ ＜  E~x属于源域公共类~H(y^^^) -  E~x属于源域公共类~d^^^’ 

E~x属于源域私有类~w^s^(x)logD(F(x)) ＜  E~x属于源域公有类~w^s^(x)logD(F(x))

就是说损失函数更关注于x属于源域公有类这部分。

情况二：x是目标域多出类别的样本和x是目标域公共类别的样本，那么：

E~x属于目标域私有类~d^^^’  - E~x属于目标域私有类~H(y^^^) ＜  E~x属于目标域公有类~d^^^’  - E~x属于目标域公有类~H(y^^^) 

E~x属于源域私有类~w^t^(x)logD(F(x)) ＜  E~x属于源域公有类~w^t^(x)logD(F(x))

就是说损失函数更关注于x属于目标域公有类这部分。

