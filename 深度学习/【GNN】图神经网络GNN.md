# 图神经网络

原文地址：https://distill.pub/2021/gnn-intro/



## 1. 图的构造

![image-20220823140205277](D:\markdown file\截图\image-20220823140205277.png)


![image-20220823140335043](D:\markdown file\截图\image-20220823140335043.png)



## 2. 图的tensor表示

![image-20220823140415641](D:\markdown file\截图\image-20220823140415641.png)



## 3. 将数据表示为图

* **将图像表示为图**

![image-20220823140507902](D:\markdown file\截图\image-20220823140507902.png)

* **将文本表示为图**

![image-20220823140614906](D:\markdown file\截图\image-20220823140614906.png)

* **将社交网络表示为图**

![image-20220823140715282](D:\markdown file\截图\image-20220823140715282.png)



## 4. 图神经网络的任务

* **图级别的任务**

![image-20220823144212970](D:\markdown file\截图\image-20220823144212970.png)

* **结点级别的任务**

![image-20220823144241705](D:\markdown file\截图\image-20220823144241705.png)

* **边级别的任务**

![image-20220823144315090](D:\markdown file\截图\image-20220823144315090.png)



## 5. 可以将邻接矩阵表示为邻接列表

![image-20220823140853995](D:\markdown file\截图\image-20220823140853995.png)



## 6. 图神经网络的介绍

### 6.1 信息传递(等价于GCN的时域理解)

![image-20220830125934167](D:\markdown file\截图\image-20220830125934167.png)



## 7. 图神经网络的几种训练方式

![image-20220827121751477](D:\markdown file\截图\image-20220827121751477.png)

![image-20220827121957614](D:\markdown file\截图\image-20220827121957614.png)



### 7.1 基于空间的图神经网络 spatial-based

![image-20220823154806211](D:\markdown file\截图\image-20220823154806211.png)



#### 7.1.1 NN4G

* 先把原结点v~3~的张量x~3~乘以w得到h~3~^0^（其余节点同理），再把h~3~^0^的邻居结点h~0~^0^、h~2~^0^、h~4~^0^加起来乘以w，与h~3~^0^相加得到h~3~^1^
* 把h~i~^0^相加取平均乘以w，h~i~^1^相加取平均乘以w，再把相乘的结果相加得到这个图的feature

![image-20220823154956883](D:\markdown file\截图\image-20220823154956883.png)

![image-20220823155238709](D:\markdown file\截图\image-20220823155238709.png)



#### 7.1.2 DCNN

* 把与原节点距离是1的结点加起来取平均乘以w，再把与原节点距离是2的结点加起来取平均乘以w（注意加的都是第一层的）
* 最后生成一个特征矩阵，将特征矩阵叠起来与w相乘得到这个结点的feature

![image-20220823162500006](D:\markdown file\截图\image-20220823162500006.png)

![image-20220823162654735](D:\markdown file\截图\image-20220823162654735.png)

![image-20220823162728453](D:\markdown file\截图\image-20220823162728453.png)



#### 7.1.3 DGC

* DGC就是将DCNN中特征图的concat变为加和

![image-20220827132943456](D:\markdown file\截图\image-20220827132943456.png)



#### 7.1.4 MoNET

* 将邻居节点之间加入了权重，也叫距离

![image-20220827133353832](D:\markdown file\截图\image-20220827133353832.png)



#### 7.1.5 GAT

* 对邻居节点做attention

![image-20220827133800823](D:\markdown file\截图\image-20220827133800823.png)

* GAT在图卷积网络中加入自注意力机制

![image-20220823163359210](D:\markdown file\截图\image-20220823163359210.png)



#### 7.1.6 GIN

* GIN表明处理邻居结点时最好用sum而不是max或者mean，处理自己的结点时乘以一个系数即可

![image-20220823163727708](D:\markdown file\截图\image-20220823163727708.png)



### 7.2.基于频域的图神经网络 spectral-based

#### 7.2.1 由CNN到GCN

![image-20220823164850951](D:\markdown file\截图\image-20220823164850951.png)

![image-20220823165004807](D:\markdown file\截图\image-20220823165004807.png)



#### 7.2.2 为什么用拉普拉斯矩阵

##### 7.2.3.1 数量场、向量场、梯度和散度

* **数量场和向量场**

![image-20220827154709945](D:\markdown file\截图\image-20220827154709945.png)



* **数量场的两种表示方式：图像和等高图**

图像要表示二维数据必须要变为三维，比如：f(x, y) = x^2^ + y^2^

![image-20220827154904603](D:\markdown file\截图\image-20220827154904603.png)

等高图要表示二维数据不用变为三维

![image-20220827154936113](D:\markdown file\截图\image-20220827154936113.png)



* **向量场的表示方式**

![image-20220827155135465](D:\markdown file\截图\image-20220827155135465.png)



* **nabla算子作用于数量场(相当于求梯度)，将数量场变为向量场**

![image-20220827163442066](D:\markdown file\截图\image-20220827163442066.png)

![image-20220827155342115](D:\markdown file\截图\image-20220827155342115.png)

![image-20220827155418959](D:\markdown file\截图\image-20220827155418959.png)



* **nabla算子与同纬度的向量场做内积(相当于求散度、发散度)，将向量场变为数量场**

![image-20220827163458945](D:\markdown file\截图\image-20220827163458945.png)

![image-20220827155433315](D:\markdown file\截图\image-20220827155433315.png)

![image-20220827160003581](D:\markdown file\截图\image-20220827160003581.png)

![image-20220827161251830](D:\markdown file\截图\image-20220827161251830.png)

![image-20220827161442986](D:\markdown file\截图\image-20220827161442986.png)

散度是通量的局部描述，比如给定一点，画出一个包含该点的小区域，这个区域既有向量流入，又有向量流出，如果流出的更多，通量为正，就是场中有正源，就好像喷泉一样往外喷水，反之如果流出的少，通量为负，就是场中有负源，就好像一个洞。求出单位面积或者单位体积上的通量，再让这个区域收缩到这个点，那么这个平均通量就存在一个极限，就是该点处的散度，所以散度也叫做通量的体密度。



![image-20220828132621445](D:\markdown file\截图\image-20220828132621445.png)

![image-20220828132631886](D:\markdown file\截图\image-20220828132631886.png)



##### 7.2.3.2 拉普拉斯算子

* **解释一**

![image-20220828132757577](D:\markdown file\截图\image-20220828132757577.png)



* **解释二**

![image-20220827163301656](D:\markdown file\截图\image-20220827163301656.png)

![image-20220827164306695](D:\markdown file\截图\image-20220827164306695.png)

![image-20220828124018642](D:\markdown file\截图\image-20220828124018642.png)

这里的公式也可以写为

Δf = f(x+1, y) - f(x, y) + f(x-1, y) - f(x, y) + f(x, y+1) - f(x, y) + f(x, y-1) - f(x, y)

或者

Δf = f(x+1, y) - f(x, y) - [ f(x, y) - f(x-1, y) ]+ f(x, y+1) - f(x, y) - [ f(x, y) - f(x, y-1) ]

就是x方向的梯度变化率 + y方向的梯度变化率

![image-20220828124035984](D:\markdown file\截图\image-20220828124035984.png)

![image-20220828124053032](D:\markdown file\截图\image-20220828124053032.png)

箭头表示上山的方向，q1，q2，q4相当于山谷，q3相当于山峰

![image-20220828133232551](D:\markdown file\截图\image-20220828133232551.png)

对于一个函数，拉普拉斯算子实际上衡量了在空间中的每一点处，该函数梯度是倾向于增加还是减少

简单来说，拉普拉斯算子计算了中心点和周围点相互转换的关系，如果Δf＞0就会由中心点变为周围点，如果Δf＜0就会由周围点变为中心点，如果Δf=0大家都不变。



就是说你站在山上，想上山

在山谷处，你会走上坡，你按照最陡的方向走，散度大于0，此时梯度倾向于增加，你站的山谷也叫做正源，散发梯度。

在山顶处，你也是走上坡，你也按照最陡的方向走，但是山顶是个平面，再陡峭也没有你从山谷到山顶过程中陡峭，散度小于0，此时梯度倾向于减少，你站的山谷也叫做负源，吸收梯度。



##### 7.2.3.3 从拉普拉斯算子到拉普拉斯矩阵

![image-20220828125514053](D:\markdown file\截图\image-20220828125514053.png)



![image-20220828130159869](D:\markdown file\截图\image-20220828130159869.png)

等价于

Δf~i~ = w~i1~(f~i~ - f~1~) + w~i2~(f~i~ - f~2~) + w~i3~(f~i~ - f~3~) + w~i4~(f~i~ - f~4~) + w~i5~(f~i~ - f~5~)

就是下面这个公式的改写

![image-20220828130513727](D:\markdown file\截图\image-20220828130513727.png)

等价于

Δf = f(x+1, y) - f(x, y) + f(x-1, y) - f(x, y) + f(x, y+1) - f(x, y) + f(x, y-1) - f(x, y)

Δf~i~ = - [ 1(f~i~ - f~1~) + 1(f~i~ - f~2~) + 1(f~i~ - f~3~) + 1(f~i~ - f~4~) ]



![image-20220828131628794](D:\markdown file\截图\image-20220828131628794.png)

![image-20220828131805346](D:\markdown file\截图\image-20220828131805346.png)

![image-20220828152743316](D:\markdown file\截图\image-20220828152743316.png)



![image-20220828161633887](D:\markdown file\截图\image-20220828161633887.png)

Lf = Δf = [ Δf~1~, Δf~2~, Δf~3~, ... , Δf~N~]

Δf~i~ = w~i1~(f~i~ - f~1~) + w~i2~(f~i~ - f~2~) + w~i3~(f~i~ - f~3~) + w~i4~(f~i~ - f~4~) + w~i5~(f~i~ - f~5~)

![image-20220828164253119](D:\markdown file\截图\image-20220828164253119.png)



#### 7.2.3 谱图理论

* **傅里叶变换简介**

![image-20220827135309932](D:\markdown file\截图\image-20220827135309932.png)



* **图论简介**

![image-20220827135417091](D:\markdown file\截图\image-20220827135417091.png)

G=图，V=所有节点的集合，E=边的集合，N=节点数，A=邻接矩阵，D=度矩阵(度表示一个节点有几个邻居)，f=节点的向量，代表节点的信息，类似于word embedding



* **拉普拉斯矩阵的谱分解**

![image-20220827140242384](D:\markdown file\截图\image-20220827140242384.png)

![image-20220827140617421](D:\markdown file\截图\image-20220827140617421.png)

特征值是频率，特征向量是基

![image-20220823170332852](D:\markdown file\截图\image-20220823170332852.png)

无论什么图，他的特征值最小只能是0

![image-20220823170415848](D:\markdown file\截图\image-20220823170415848.png)

特征值是频率，特征向量是基，这里其实可以理解为特征向量内元素之间的差距，比如：λ=0时，其特征向量[0.5, 0.5, 0.5, 0.5]之间的差距是0， λ=1时，其特征向量[-0.41, 0, -0.41, 0.82]之间的差距是0.41

![image-20220827143238934](D:\markdown file\截图\image-20220827143238934.png)



#### 7.2.4 由傅里叶变换到图傅里叶变换

![image-20220828165204076](D:\markdown file\截图\image-20220828165204076.png)

![image-20220829141415707](D:\markdown file\截图\image-20220829141415707.png)



#### 7.2.5 图的频域相乘等于时域做卷积

![image-20220829141954018](D:\markdown file\截图\image-20220829141954018.png)

g~θ~(⋀) = diag [g~θ~(λ~1~), g~θ~(λ~2~), g~θ~(λ~3~), ... , g~θ~(λ~N~)] ，N是节点数，这里有一个大问题，卷积核的参数和输入图的大小相关，输入的图节点数越大，卷积核越大。

g~θ~(⋀)定义为一个在频域上的滤波器，这样转换到时域就起到了卷积核的作用



![image-20220829142623077](D:\markdown file\截图\image-20220829142623077.png)

![image-20220826141606356](D:\markdown file\截图\image-20220826141606356.png)

先对x做图傅里叶变换U^T^x变到频域，再在频域乘以一个滤波器g~θ~(⋀)，最后将过滤波器的信号在做图傅里叶逆变换Ux转换到时域。这样就完成了图在时域经过卷积核的操作。



![image-20220829143422448](D:\markdown file\截图\image-20220829143422448.png)

最终图在时域经过卷积核相当于左乘一个g~θ~(L)，其中g~θ~( · )是一个任意的函数，L是拉普拉斯矩阵。

这里有第一个大问题，卷积核的参数和输入图的大小相关，输入的图节点数越大，卷积核越大。



![image-20220829151534557](D:\markdown file\截图\image-20220829151534557.png)

其次，如果考虑函数g~θ~(L) = L，那么可以看到对于第一个节点V~0~的图卷积操作并没有考虑到节点V~3~，因为他们没有连接。

![image-20220829151829447](D:\markdown file\截图\image-20220829151829447.png)

如果考虑函数g~θ~(L) = L^2^，那么可以看到对于第一个节点V~0~的图卷积操作就考虑到节点V~3~了，即使他们没有连接。

![image-20220829152108350](D:\markdown file\截图\image-20220829152108350.png)

有一个定理，如果考虑函数g~θ~(L) = L^N^，N是节点数，那么卷积就可以考虑到所有的节点，无论节点之间有没有连接。但是这样做就没有局部性了。

![image-20220829152327660](D:\markdown file\截图\image-20220829152327660.png)

考虑一下CNN的卷积，卷积核的大小影响感受野的大小，就是说函数g~θ~(L)不是乱选的，我们要在图卷积上加入局部性。



#### 7.2.6 ChebNet

![image-20220829142623077](D:\markdown file\截图\image-20220829142623077.png)

![image-20220829154335440](D:\markdown file\截图\image-20220829154335440.png)

![image-20220829162801796](D:\markdown file\截图\image-20220829162801796.png)

对比之前的g~θ~(⋀) = diag [g~θ~(λ~1~), g~θ~(λ~2~), g~θ~(λ~3~), ... , g~θ~(λ~N~)]  = diag [ θ~1~, θ~2~, θ~3~, ..., θ~N~ ] ≈ diag [ θ~1~λ~1~, θ~2~λ~2~, θ~3~λ~3~, ..., θ~N~λ~N~ ] 相当于为每个特征值λ都分配了一个可学习参数θ，其中N是节点数。原始的做法由两个问题，一个是有N个节点就有N个参数，参数与节点数成正比；二个是没有考虑卷积核的局部性，函数g~θ~(L) = L^N^就考虑所有的节点了。

ChebNet的g~θ~(⋀) =  θ~1~⋀ + θ~2~⋀^2^ + θ~3~⋀^3^ + ... + θ~K~⋀^K^ = diag [ θ~1~λ~1~+θ~2~λ~1~^2^+θ~3~λ~1~^3^ + ... +θ~K~λ~1~^K^, θ~1~λ~2~+θ~2~λ~2~^2^+θ~3~λ~2~^3^ + ... +θ~K~λ~2~^K^, θ~1~λ~3~+θ~2~λ~3~^2^+θ~3~λ~3~^3^ + ... +θ~K~λ~3~^K^, ... , θ~1~λ~N~+θ~2~λ~N~^2^+θ~3~λ~N~^3^ + ... +θ~K~λ~N~^K^] 解决了上述的2个问题，一是将参数的数量改为了K，这个K是超参数可控；二是将g~θ~(L) = L^N^的次方数限制为g~θ~(L) = L^K^，这个K可控，这样就同时解决了参数数量随着图节点增大和卷积核局部性的问题

但是这样做也产生了第三个问题，就是时间复杂度太高。



![image-20220829162506285](D:\markdown file\截图\image-20220829162506285.png)

![image-20220829162537097](D:\markdown file\截图\image-20220829162537097.png)

解决第三个问题简单来说就是引入切比雪夫不等式，减少计算复杂度，了解即可。



#### 7.2.7 GCN

![image-20220829164704900](D:\markdown file\截图\image-20220829164704900.png)

GCN就是一阶ChebNet

![image-20220829173515209](D:\markdown file\截图\image-20220829173515209.png)

GCN层数越多效果越差，加入DropEdge可以改善，但是还是不能改变层数越多效果越差这种结果。



#### 7.2.8 从时域角度理解GCN

![image-20220829165004971](D:\markdown file\截图\image-20220829165004971.png)

![image-20220830125350730](D:\markdown file\截图\image-20220830125350730.png)

![image-20220829172416409](D:\markdown file\截图\image-20220829172416409.png)

![image-20220829172430643](D:\markdown file\截图\image-20220829172430643.png)

![image-20220829172439869](D:\markdown file\截图\image-20220829172439869.png)

![image-20220829172451896](D:\markdown file\截图\image-20220829172451896.png)

![image-20220829172500780](D:\markdown file\截图\image-20220829172500780.png)

![image-20220829172513688](D:\markdown file\截图\image-20220829172513688.png)









