## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\19.【2008】【JMLR】【Maaten、Hinton】【多伦多大学】【t-SNE可视化】Visualizing Data using t-SNE.pdf)

### 1.1 在干什么、有什么贡献、什么结论

我们提出了一种名为“t-SNE”的新技术，该技术通过在二维或三维图中给每个数据点一个位置来可视化高维数据。t-SNE是SNE的进化版。

可视化很重要，但是以前的技术对几十维的数据进行可视化效果还行，但是在面对几千维的数据进行可视化时，效果就不行了，别人不行但t-SNE行。

## 2. SNE

出发点很简单：在高维空间相似的数据点，映射到低维空间距离也是相似的。常规的做法是用欧式距离表示这种相似性，而SNE把这种距离关系转换为一种条件概率来表示相似性。

**两个概率公式：**

什么意思呢？考虑高维空间中的两个数据点xi和xj，xi以条件概率p j∣i选择xj作为它的邻近点。考虑以xi为中心点的高斯分布，若xj越靠近xi，则p j∣i越大。反之，若两者相距较远，则p j∣i极小。因此，我们可以这样定义p j∣i：

![image-20221021124222441](D:\markdown file\截图\image-20221021124222441.png)

其中σi表示以xi为中心点的高斯分布的方差。由于我们只关心不同点对之间的相似度，所以设定pi∣i=0。SNE将高维的距离（就是高维特征之间的距离）表示为相似性的概率，就是说若特征xi和特征xj接近，那么p~j|i~就大，反之就小。

当我们把数据映射到低维空间后，高维数据点之间的相似性也应该在低维空间的数据点上体现出来。这里同样用条件概率的形式描述，假设高维数据点xi和xj在低维空间的映射点分别为yi和yj。类似的，低维空间中的条件概率用q j∣i表示，并将所有高斯分布的方差均设定为1/√2，所以有：

![image-20221021124632695](D:\markdown file\截图\image-20221021124632695.png)

高维特征xi对应低维特征yi，高维特征xj对应低维特征yj，若特征yi和特征yj接近，那么q~j|i~就大，反之就小。同理，设定q i∣i=0。此时就很明朗了，若yi和yj真实反映了高维数据点xi和xj之间的关系，那么条件概率p j∣i与q j∣i应该完全相等。这里我们只考虑了xi与xj之间的条件概率，若考虑xi与其他所有点之间的条件概率，则可构成一个条件概率分布Pi，同理在低维空间存在一个条件概率分布Qi且应该与Pi一致。

如何衡量两个分布之间的相似性？当然是用经典的KL距离(Kullback-Leibler Divergence)，SNE最终目标就是对所有数据点最小化这个KL距离，我们可以使用梯度下降算法最小化如下代价函数：

![image-20221021125131538](D:\markdown file\截图\image-20221021125131538.png)

到这里，就将高维特征转为低维特征的问题就转换为，将高维特征的相似性概率p~j|i~转换到低维特征q~j|i~的相似性概率的问题。似乎到这里问题就漂亮的解决了，你看我们代价函数都写出来了，剩下的事情就是利用梯度下降算法进行训练了。

**但事情远没有那么简单，因为KL距离是一个非对称的度量。**最小化代价函数的目的是让p j∣i和q j∣i的值尽可能的接近，即低维空间中点的相似性应当与高维空间中点的相似性一致。**但是从代价函数的形式就可以看出，当p j∣i较大，q j∣i较小时，代价较高，而p j∣i较小，q j∣i较大时，代价较低。**什么意思呢？很显然，高维空间中两个数据点距离较近时，若映射到低维空间后距离较远，那么将得到一个很高的惩罚，这当然没问题。**反之，高维空间中两个数据点距离较远时，若映射到低维空间距离较近，将得到一个很低的惩罚值，这就有问题了，理应得到一个较高的惩罚才对。**换句话说，SNE的代价函数更关注局部结构，而忽视了全局结构。

![image-20221023135214837](D:\markdown file\截图\image-20221023135214837.png)

SNE算法中还有一个细节是关于高维空间中以点xi为中心的正态分布方差σi的选取。那么σi是如何选取的呢？首先σi不太可能是一个最优值，比如对于高维特征密度大的区域，小σi比大σi更合适，所以要根据密度来选。如果将所有样本点都参与计算，那么对于每个σi，都可以产生一个分布Pi , 这个分布的熵随着σi的增加而增加（就是σi越大，高斯分布越胖，不确定性越高，极端情况高斯分布是一条线，那么取每个点的概率都相同了），所以定义一个困惑度（perplexity）:

![image-20221021131734738](D:\markdown file\截图\image-20221021131734738.png)

![image-20221021131742919](D:\markdown file\截图\image-20221021131742919.png)

困惑度可以解释为有效邻域数的度量，通常是5-50的值。

SNE代价函数对yi求梯度后的形式如下：

![image-20221021131857040](D:\markdown file\截图\image-20221021131857040.png)

## 3. t-SNE

SNE算法的思路是不错的，但是它的可视化效果大家也看到了，存在很大改进空间。如何改进它呢？我们一步一步来，先看看如何解决SNE中的不对称问题。

**对称SNE：**

[视频连接](https://www.bilibili.com/video/BV1cU4y1w74A/?spm_id_from=333.337.search-card.all.click)

![image-20221022200630976](D:\markdown file\截图\image-20221022200630976.png)

在原始的SNE中，p i∣j与p j∣i是不相等的，低维空间中q i∣j与q j∣i也是不相等的。所以如果能得出一个更加通用的联合概率分布更加合理，即分别在高维和低维空间构造联合概率分布P和Q，使得对任意 i,j，均有p~ij~=p~ji~，q~ij~=q~ji~。**所以说我们用联合概率来代替条件概率，条件概率p~j|i~的意思是在以xi为中心点的情况下，取到xj的概率，而联合概率p~ij~的意思是取到xi和xj的概率，这两个概率的分母是不同的。对于条件概率，我们的分母应该是以xi为中心点的情况下，取到其他点的概率；而对于联合概率，我们的分母应该是取到其他两个不同点的概率。**

在低维空间中，我们可以这样定义q~ij~：

![image-20221022155752331](D:\markdown file\截图\image-20221022155752331.png)

在高维空间呢？是不是可以想当然的写出：

![image-20221022155810260](D:\markdown file\截图\image-20221022155810260.png)

**但是如果这样定义p~ij~又会遭遇问题，考虑一个离群点xi，它与所有结点之间的距离都较大，那么对所有j，p~ij~的值均较小，所以无论该离群点在低维空间中的映射点yi处在什么位置，惩罚值都不会太高**，这显然也不是我们希望看到的。**所以这里采用一种更简单直观的方式定义p~ij~：**

![image-20221023005722997](D:\markdown file\截图\image-20221023005722997.png)

![image-20221022200921608](D:\markdown file\截图\image-20221022200921608.png)

其中n为数据点的总数，这样定义即满足了对称性，又保证了xi的惩罚值不会过小。此时可以利用KL距离写出如下代价函数：

![image-20221022201123766](D:\markdown file\截图\image-20221022201123766.png)梯度变为：

![image-20221022201151222](D:\markdown file\截图\image-20221022201151222.png)

相比刚才定义的公式，这个梯度更加简化，计算效率更高。但是别高兴的太早，虽然我们解决了SNE中的不对称问题，得到了一个更为简单的梯度公式，但是，对称SNE的效果只是略微优于原始SNE的效果，依然没有从根本上解决问题。

**拥挤问题**：

所谓拥挤问题，顾名思义，看看SNE的可视化效果，不同类别的簇挤在一起，无法区分开来，这就是拥挤问题。有的同学说，是不是因为SNE更关注局部结构，而忽略了全局结构造成的？这的确有一定影响，但是别忘了使用对称SNE时同样存在拥挤问题。实际上，拥挤问题的出现与某个特定算法无关，而是由于高维空间距离分布和低维空间距离分布的差异造成的。

我们生活在一个低维的世界里，所以有些时候思维方式容易受到制约。比如在讨论流形学习问题的时候，总喜欢拿一个经典的“Swiss roll”作为例子，这只不过是把一个简单的二维流形嵌入到三维空间里而已。实际上真实世界的数据形态远比“Swiss roll”复杂，比如一个10维的流形嵌入到更高维度的空间中，现在我们的问题是把这个10维的流形找出来，并且映射到二维空间上可视化。在进行可视化时，问题就来了，在10维流形上可以存在11个点且两两之间距离相等。在二维空间中呢？我们最多只能使三个点两两之间距离相等，**想将高维空间中的距离关系完整保留到低维空间是不可能的。**

**如何解决呢？这个时候就需要请出t分布了**

**t-SNE**：

像t分布这样的长尾分布，在处理小样本和异常点时有着非常明显的优势，在没有异常点时，t分布与高斯分布的拟合结果基本一致。而在出现了部分异常点的情况下，由于高斯分布的尾部较低，对异常点比较敏感，为了照顾这些异常点，高斯分布的拟合结果偏离了大多数样本所在位置，方差也较大。相比之下，t分布的尾部较高，对异常点不敏感，保证了其鲁棒性，因此其拟合结果更为合理，较好的捕获了数据的整体特征。 那么如何利用t分布的长尾性来改进SNE呢？我们来看下面这张图，注意这个图并不准确，主要是为了说明t分布是如何发挥作用的。

![image-20221022205530554](D:\markdown file\截图\image-20221022205530554.png)

图中有高斯分布和t分布两条曲线，表示点之间的相似性与距离的关系，高斯分布对应高维空间，t分布对应低维空间。**那么对于高维空间中相距较近的点，为了满足p~ij~=q~ij~，低维空间中的距离需要稍小一点；而对于高维空间中相距较远的点，为了满足p~ij~=q~ij~，低维空间中的距离需要更远。**这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。我们使用自由度为1的t分布重新定义q~ij~：

![image-20221022205250716](D:\markdown file\截图\image-20221022205250716.png)

**这就是所谓的t-SNE算法，总结一下其实就是在SNE的基础上增加了两个改进：一是把SNE变为对称SNE，二是在低维空间中采用了t分布代替原来的高斯分布，高维空间不变。**

**总结：最后的概率公式如下，高维数据用高斯分布，低维数据用t分布。**

![image-20221021124222441](D:\markdown file\截图\image-20221021124222441.png)

![image-20221023004920297](D:\markdown file\截图\image-20221023004920297.png)

在t-SNE中，我们用高斯分布描述了高维空间中整体的距离分布关系，注意表达式是这样的一个形式：

![image-20221023005155640](D:\markdown file\截图\image-20221023005155640.png)

对每一个数据点都包含∑~k≠i~exp(−|x~i~−x~k~|^2^/2σ^2^~i~)这样一项，当数据量较大时，计算量无疑是非常大的。但实际上，两个相距较远的点互为邻居的概率p~ij~是非常小的，几乎可以忽略。**因此，在高维空间对一个点构建距离相似性关系时，不必考虑图中的每一个节点，只需考虑与其相近的若干个节点即可。**

![image-20221023005338060](D:\markdown file\截图\image-20221023005338060.png)

**具体实现：**

![image-20221023005630231](D:\markdown file\截图\image-20221023005630231.png)

![image-20221023010514153](D:\markdown file\截图\image-20221023010514153.png)

**个人理解：**

[图视频链接](https://www.bilibili.com/video/BV1va411m74T/?spm_id_from=333.788.header_right.fav_list.click&vd_source=320c7991448cfd9ab61c95f538663e07)

![image-20221022145545308](D:\markdown file\截图\image-20221022145545308.png)

![image-20221022145631841](D:\markdown file\截图\image-20221022145631841.png)

![image-20221022145743294](D:\markdown file\截图\image-20221022145743294.png)

**(1) p j∣i 和 q j∣i 的公式为什么是那种形式？**

首先呢，先看分子，就是以xi为正态分布中心，计算取到xj点的概率。分母就是以xi为正态分布中心，计算取到xk(k≠j)点的概率，也就是取到其他点的概率，两个概率一除，正态分布原来的分母项就被除掉了，只剩分子，所以写成上面的形式。

作者还假设 p i∣i=0 和 q i∣i=0，因为我们要估计的是xi到xj的距离（这种距离以概率的方式表现）xi到xi的距离对于我们来说没有意义，所以置为0。

具体可以看上面的**第一幅图**，非常形象。换个角度p j∣i 和 q j∣i 其实也可以看作是经过softmax后的概率值，所以写成这种形式**最大的目的就是为了对相似度进行归一化**。

**(2) 为什么要对相似度进行归一化？**

首先呢，我们可以看到p j∣i 的公式中有σi项，这就证明了对于高纬度特征，每个以xi为中心的正态分布都是不同的（通俗来讲就是有的胖有的瘦，比如**第二幅图**）。如果不对相似度进行归一化，那么对于**第三幅图**来说，我们以0.05为阈值，对于瘦的分布我们可以保留0.24和0.05两个点，对于胖的分布我们只能保留0.12这一个点，但是对于我们来说，胖的分布中0.024这个点和瘦的分布中0.05这个点同等重要，我们也要保留，所以我们要进行相似度的归一化。那么归一化后0.24和0.24这个两个点变为0.82，0.05和0.024这两个点变为0.12，如果我们再以0.12为阈值就可以同时保留这两个点。

**(3) 为什么p j∣i 有 σi 而 q i∣i 直接令 σi = 1/√2？**

首先，σi是以数据点xi为中心的高斯函数的方差，σi项控制的是高斯分布的胖瘦。对于高维数据来说，由于特征密度的不同，其特征的高斯分布的胖瘦也不同，对于密度高的特征高斯分布瘦，密度低的特征高斯分布胖，我们并不能控制，比如**第二幅图**，所以在高维数据中，σi项不饿能给一个确定的数值，要去计算。但是对于低维特征来说，低维特征是我们人为生成，所以我们自然也可以控制其高斯分布的胖瘦，如果我们将所有分布的σi项定为一个确定值，那么我们生成的特征密度也是差不多的，这样美观又进行了密度上的归一化。



