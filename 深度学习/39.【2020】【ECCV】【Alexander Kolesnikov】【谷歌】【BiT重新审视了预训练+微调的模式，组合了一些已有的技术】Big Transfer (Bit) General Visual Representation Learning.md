## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\39.【2020】【ECCV】【Alexander Kolesnikov】【谷歌】【BiT重新审视了预训练+微调的模式，组合了一些已有的技术】Big Transfer (Bit) General Visual Representation Learning.pdf)

### 1.1 在干什么、有什么贡献、什么结论

在视觉领域，预训练+微调的模式仍然是很多任务上SoTA效果的主流，BiT（Big Transfer)重新审视了这个模式，组合了一些已有的技术，在20+个数据集上达到了很好的效果，不仅如此，在小样本数据集上同样成就斐然。

对于预训练+微调模式的迁移学习来说，更有效的结果分为两种：1、在某一类下游任务中取得优秀的性能。2、能够完成尽可能多的下游任务，且都能够取得较为良好的成绩。

对于BiT来说，属于第二个，在近20个数据集上完成较好的结果，并且最关键的是，BiT在迁移的过程使用的是固定（相同）的处理方法，从而更能说明其具有较好的普适性。


## 2. 细节

### **上游预训练：**

在上游预训练阶段，采用了如下几个技术：

- 大规模，一般来说，数据量越多，效果越好，但需要的模型尺寸也就越大。因此，BiT在ILSVRC-2012（1.3M图片）, ImagegNet-21k（14M图片）和JFT（300M图片）数据集上分别做了实验，来探究计算量，模型尺寸和数据量之间的相互作用。

- GN(Group Normalization)，BN(Batch Normalization)是稳定训练的常用技术，但却对BiT有害，主要原因有二：

- - 模型尺寸比较大的时候，在每个device上batch size无法太大。因而需要设备间通信，这样性能很差
  - BN不适合迁移学习，因为数据变了统计量往往需要变化。

- WS(Weight Standardization)，WS + GN可以在ImageNet和CoCo的小batch训练上个提升性能。

**Group Normalization**

Group Normalization与Batch Normalization以及其他的Normalization的区别可以看下图：

<img src="D:\markdown file\截图\image-20221125134741722.png" alt="image-20221125134741722" style="zoom:50%;" />

相比于BN来说，GN的特点是与batch解绑，不会收到batch size的影响。当batch size发生变化时，GN可以在小batch上击败BN。

<img src="D:\markdown file\截图\image-20221125134815993.png" alt="image-20221125134815993" style="zoom:50%;" />

### **下游迁移：**

在下游任务的迁移上，找到了如下几个重要因素：

- training schedule length
- 图片清晰程度
- 是否使用Mix-up正则化

一些尝试了但是没有采用的技术如下：

- 权重衰减到0 (weight decay to zero)
- 权重衰减到初始化参数 （weight decay to initial parameters)
- dropout

**Mix-up Regularization**

关于Mix-up, 它的做法其实非常简单，就是将输入图像和输出的label同时做加权平均，本质上看，是一种数据增强的方法，将数据点之间的模糊地带给填上了。

在BiT的实验中，发现Mix-up在预训练阶段没有用，因为预训练阶段图像足够多。在transfer阶段，发现在mid-size的数据集上最有用，在few-shot的数据集上用处不大。