## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\10.【2017】【ICCV】【Saeid Motiian】【西弗吉尼亚大学】【给出了一种基于线性距离度量和相似性度量的损失减小域之间的差异】Unified Deep Supervised Domain Adaptation and Generalization.pdf)

### 1.1 在干什么、有什么贡献、什么结论

**问题研究背景**：supervised domain adaptation(SDA)，源域有大量带标签的数据，目标域仅有少量可使用的数据

网络结构大致不变，实际上主要的贡献是CCSA损失函数。

**实际上最主要的贡献就是，给出了一种基于距离度量和相似性度量的损失减小域之间的差异。**

## 2.网络结构

![image-20221006123330711](D:\markdown file\截图\image-20221006123330711.png)

非对抗的方法，利用两个同样的网络（Siamese网络），分别对源域和目标域编码，最后在Loss层优化两个编码的相似性。使源域数据分布和目标域分布在语义上对齐。

**总loss：**

<img src="D:\markdown file\截图\image-20221009150714785.png" alt="image-20221009150714785" style="zoom:67%;" />

**主要包含3个Loss:**

<img src="D:\markdown file\截图\image-20221006123236499.png" alt="image-20221006123236499" style="zoom: 67%;" />

<img src="D:\markdown file\截图\image-20221006130103923.png" alt="image-20221006130103923" style="zoom:67%;" />

**语义对齐loss（3）** **最小化** 来自**不同domain的同一类样本之间的距离，也就是类别对齐** ，d是嵌入空间中的某种距离度量，比如欧式距离。实际中用（7）代替（3）进行计算。值得注意的是**对不同domain的同一类样本进行操作**。

<img src="D:\markdown file\截图\image-20221006123542017.png" alt="image-20221006123542017" style="zoom:67%;" />

<img src="D:\markdown file\截图\image-20221006130219154.png" alt="image-20221006130219154" style="zoom: 67%;" />

**分隔loss （4）最大化** 来自**不同domain的不同类的样本之间的距离**，k是某种相似性度量（分隔loss最大化等同于让相似度最小）。实际中用（8）代替（4）进行计算。值得注意的是**对不同domain的不同类样本进行操作**。



<img src="D:\markdown file\截图\image-20221006123701807.png" alt="image-20221006123701807" style="zoom:67%;" />

**分类loss**保证识别准确率。

<img src="D:\markdown file\截图\image-20221006131745529.png" alt="image-20221006131745529" style="zoom: 67%;" />

距离度量k采用欧氏距离。
相似性度量采用（10）式的方法，m表示分隔度（超参数），当两个分布很相似时，k接近m；当不相似时，k接近0。

## 3. 域对齐和语义对齐（也叫类别对齐）

这里我们简单理解下何为域对齐以及语义对齐。

所谓**域对齐**，就是两个来自不同域的数据源通过特征映射后，映射到相同特征空间的**某一个区域中**，在这个区域内，两个数据源的域靠的很近。

那么仅仅域间靠的很近就可以了吗？作者提到即使两个域足够接近 ，也无法保证出于不同域的同一个类物体被分到**相邻/相同的区域**。这里作者给出了一个MNIST-USPS数据集的例子。

![image-20221009150946972](D:\markdown file\截图\image-20221009150946972.png)

**左图**就是两个域的向量分布（M和U），可见M和U之间域还是挺明显的（M集中在右上，U集中在左下）。

作者引入最基本的domain adaptation后（如**中间图**所示），虽然两个域之间的距离更小了（实现了基本的domain alignment），但是不同域同类物体还是相差挺远，比如1M和1U。

接着作者在此基础上引入了两个损失函数（分别为semantic alignment loss和separation loss）实现所谓的**语义对齐（**semantic alignment**）**，获得如**右图**所示的结果，不同域相同类被分到了相邻的区域中。

![image-20221009151120024](D:\markdown file\截图\image-20221009151120024.png)

有关这两个损失函数，其实某种意义上来说就是metric learning中的contrastive loss，主要作用就是**拉近类内距离（**semantic alignment**）**的同时，**拉远类间距离（**separation loss**）**。这里我们给出下图加深各位印象（相同形状是相同class，相同颜色是相同域）。
