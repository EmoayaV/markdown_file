## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\33.【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy.pdf)

### 1.1 在干什么、有什么贡献、什么结论

本文的背景是半监督的DA任务，semi-supervised domain adaptation (SSDA)。除此之外，作者还假设拥有少量的有标签的目标域样本，因此这还是一个few-shot DA的任务。

作者提出了最大熵的方法Minimax Entropy(MME)。

**有点对比学习的意思，重要的是构造了W（权重矩阵），这就有点类似于对比学习中的字典**

## 2.网络

<img src="D:\markdown file\截图\image-20221121182942944.png" alt="image-20221121182942944" style="zoom: 50%;" />

正向传播：首先网络主要由特征提取器F和分类器C组成，样本首先经过特征提取器F，接着再经过L2正则化的操作，然后将正则化后的特征放入分类器，其中分类器的W是权重参数矩阵，T是温度，最后经过softmax操作得到一组概率值。

反向传播：首先对于有标签的样本用交叉熵损失进行训练，使其损失函数最小(就是熵最小)。对于无标签的样本，我们首先假设每个类都存在一个聚类中心(或者叫代表性的特征点)，针对目标域无标记数据计算条件熵，然后增加熵值，则每个类输出的概率值很平均。此时，固定F，更新C的参数，能让原型 wi 向未标记的目标域移动，具体如下图所示。

(这里可以这么理解，实际上 wi · f 可以看为特征经过神经网络，也可以看作向量 wi 和 f 的相似度，也就是说两者越相似，值就越大，经过softmax后概率值就越大，所以wi可以看作源域一个类别的原型，熵最大化实际上就是让wi往目标域的数据上偏一点)


<img src="D:\markdown file\截图\image-20221121184413201.png" alt="image-20221121184413201" style="zoom:50%;" />

其中损失函数如下：

<img src="D:\markdown file\截图\image-20221122133358871.png" alt="image-20221122133358871" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221122133418257.png" alt="image-20221122133418257" style="zoom:50%;" />

第一个损失用于有标签的源域、目标域数据，主要是用来减小熵值，使相似类别的样本聚类，第二个损失用于无标签的目标域数据，主要是用来增大熵值，使每个类别对应的原型wi向目标域无标签样本偏移。

总之，我们的方法可以表述为C和F之间的对抗学习。训练任务分类器C以最大化熵，而训练特征提取器F以最小化熵。

## 创新点：

首先本文将分类器的每一个权重wi当作每一个类别的原型，这样一个特征输入进来实际上就是与这个原型向量wi计算相似度。

使用了最大化熵使类别原型wi向目标域无标签样本偏移。
