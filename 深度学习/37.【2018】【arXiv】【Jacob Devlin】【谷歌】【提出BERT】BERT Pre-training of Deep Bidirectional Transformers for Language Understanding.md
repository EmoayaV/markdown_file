## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\37.【2018】【arXiv】【Jacob Devlin】【谷歌】【提出BERT】BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf)

[文章解读](https://blog.csdn.net/zhaohongfei_358/article/details/126838417)

### 1.1 在干什么、有什么贡献、什么结论

提出BERT。

## 2.网络

<img src="D:\markdown file\截图\image-20221130131404624.png" alt="image-20221130131404624" style="zoom:50%;" />

BERT的结构是Transformer的Encoder。

BERT的是典型的预训练-微调的框架，整个训练也分成2步，预训练和微调。

**预训练(上游部分)：**

BERT的预训练使用的是masked language model(MLM)预训练任务，具体为随机掩盖住输入中的部分词，目标就是根据上下文来预测这些被盖住的词是什么。例如：

*输入：我正在学习深度[MASK]，目前学到了BERT一节，有点[MASK]。*

*期望输出：学习，难*


同时还使用了NSP(Next Sentence Prediction)任务，具体为给两句话，让网络做二分类任务，区分这两句话是不是一对儿。例如：

*输入：窗前明月光，疑是地上霜*

*期望输出：1*

*输入：锄禾日当午，李白吃红薯*

*期望输出：0*

实际上比较重要的任务是MLM任务，NSP任务只是为了方便NLP下游任务的输入，还有一点注意的是BERT在第一个输入的位置加入了一个CLS表示类别，这基本已经构成了VIT的全部组件。

**微调(下游部分)：**

针对具体的下游任务，接上输出层，对BERT进行微调，就可以达到SOTA结果。

## 创新点

1.单独使用Transformer的Encoder，效果不错。

2.使用masked language model(MLM)预训练任务，后续也有不少工作把这个任务用于自监督的任务，在图像领域广泛使用。

3.使预训练-微调的框架成为主流，不光在NLP

4.BERT之后的工作证明了在大量无标签数据集上训练的模型比在少量有标签的数据集上训练的模型效果更好

5.embedding方法可以学习一下，如果需要位置信息就可以像他这样做

<img src="D:\markdown file\截图\image-20221226174232585.png" alt="image-20221226174232585" style="zoom:50%;" />