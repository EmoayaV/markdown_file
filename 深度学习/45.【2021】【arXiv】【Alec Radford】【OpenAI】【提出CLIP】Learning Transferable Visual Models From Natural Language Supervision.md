## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\45.【2021】【arXiv】【Alec Radford】【OpenAI】【提出CLIP】Learning Transferable Visual Models From Natural Language Supervision.pdf)

[文章解读1](https://zhuanlan.zhihu.com/p/511460120)

[文章解读2](https://betheme.net/houduan/40857.html?action=onClick)

### 1.1 在干什么、有什么贡献、什么结论

背景：迁移学习领域、多模态、zero-shot、用对比学习（对比学习的方法属于自监督学习）的方法

问题：

1.现在CV任务的训练太固定了，基本都是去预测事先固定好的类别（比如说训练集类别有猫狗，在测试时无论有什么类别都只能预测猫狗，ImageNet就是1000个类，COCO就是80个类）这种监督方式比较受限，因为需要额外的标记数据，限制了泛化性，如果有新的类别就要再去收集数据。

2.现有CV模型大多都只能预测已知的图像类别，对于没有见过的图像类别，需要额外的信息才能识别。那么文本其实就提供了这样的额外信息。所以利用图像对应的文本数据，也许就能使模型能够分辨未见类的图像。（和DA很像）

3.最近NLP领域中出现的BERT、GPT等预训练模型表明，用大规模的无监督数据训练模型，可以在多个下游NLP任务上获得非常好的结果，有些甚至超过使用人工标注的数据训练出的针对特点任务设计的模型。而现有的CV模型基本都是基于人工标注的数据集训练的（比如ImageNet），那么仿照NLP中预训练模型，如果使用大量无监督（也就是非人工标注）的图像，CV模型能否实现突破呢？

4.NLP大模型的成功一个重要的原因是有取之不尽用之不竭的大规模低成本的无监督数据集，而CV的数据集是有限、少量、高成本的有监督数据集，这才是限制CV的真正原因，作者是这么写的。

5.之前自监督、无监督的方法主要研究的是特征学习的能力，目标是学习一种泛化性比较好的特征，那么就算学到好的特征，应用到下游任务，还是要有标签的数据进行微调，还是会产生各种各样的数据问题（不好收集、有偏移），如何训练一个模型就不在下游微调呢？答案是借助文本作为引导来做zero shot 迁移学习（做分类就是很好的例子），这种方式也叫做**prompt**（中文可以理解为提示）

**解决方法：把自然语言作为一种监督信息训练视觉模型**

1.将自然语言作为一种监督信息使用：直接从原始文本中学习图像是一个很有前途的选择，它利用了更广泛的监督来源。本意并不是解决DA问题却无意间解决了DA问题。利用text信息监督视觉任务自训练，本质就是将分类任务化成了图文匹配任务，效果可与全监督方法相当。

2.具体来说，将4亿个text - image对放入CLIP网络进行预训练，在预训练之后，使用自然语言引用学习到的视觉概念(或描述新的视觉概念)，使模型zero-shot零样本迁移到下游任务。最后在下游，通过对30多个不同的现有计算机视觉数据集进行基准测试，涵盖了OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务，研究表明这种方法的性能确实好。最牛的是zero-shot迁移到imagenet数据集上，效果和resnet50媲美。

3.之前的一些弱监督方法效果不好的原因主要在scale，即数据集的规模和模型的规模。论文的作者团队收集了一个超级大的图像文本配对的数据集，有400 million个图片文本的配对， 模型最大用了ViT-large，提出了CLIP（Contrastive Language-Image Pre-training），是一种从自然语言监督中学习的有效方法，在模型上一共尝试了8个模型，从resnet到ViT，最小模型和最大模型之间的计算量相差约100倍，迁移学习的效果基本和模型大小成正相关。尝试了30个数据集，都能和之前的有监督的模型效果差不多甚至更好。

使用自然语言作为一种监督信息的优势：

1.容易获得容易扩展，不需要再去标注数据，比如用传统方法做分类，需要先确定类别，然后去下载图片再清洗，再标注，现在只需要去下载图片和文本的配对，数据集很容易就做大了，现在的监督对象是文本，而不是N选1的标签了。

2.与大多数无监督或自监督学习方法相比，从自然语言中学习还有一个重要的优势，那就是它不仅“只是”学习一种表示，而且还将该表示与语言联系起来，从而实现灵活的零样本迁移。

3.训练的时候把图片和文本绑在了一起，学到的特征不再单是视觉特征了，而是多模态的特征，和语言连在一起以后，就很容易做zero-shot的迁移学习了。

4.它打破了categorical label的限制（在分类的时候必须要定义一个分类的列表，如果不在分类的任务之中，是没办法分出来的，或者只能分类成最相似的。）

5.当和自然语言联系到一起时，学到的特征是多模态的特征，不单单是一个视觉特征，就很容易做zero shot的迁移学习，如果只做单模态的自监督学习，就做不到这点了。

为什么用对比学习：

1.因为给定一张图片，预测对应的文本，需要逐字逐句预测文本，这个任务太难了，因为对于同一张图片会有很多不同的描述，如果用预测性的任务去预训练模型，就有太多可能性了，训练就慢。

2.对比学习加快训练速度，只要文本和图片配对就行

## 2. 网络结构

整体结构是ConVIRT的简化版本。

<img src="D:\markdown file\截图\image-20221227161606880.png" alt="image-20221227161606880" style="zoom:50%;" />

具体计算过程：

<img src="D:\markdown file\截图\image-20221226225320339.png" alt="image-20221226225320339" style="zoom: 33%;" />

CLIP是如何进行预训练的？

模型的输入是图片和文字的配对，图片输入到图片的encoder得到一些特征，文本输入到文本的encoder得到一些特征，每个traning batch里有n个图片-文本对，就能得到n个图片的特征和n个文本的特征，然后在这些特征上做对比学习，对比学习非常灵活，就需要正样本和负样本的定义，其它都是正常套路（不懂对比学习），配对的图片-文本对就是正样本，描述的是同一个东西，特征矩阵里对角线上的都是正样本，矩阵中非对角线上的元素都是负样本，n个正样本， n2−n 个负样本，有了正负样本，模型就可以通过对比学习的方式去训练了，不需要任何手工标注。这种无监督的训练方式，是需要大量的训练数据的。

**CLIP是如何做zero-shot的推理的？**

预训练之后只能得到文本和图片的特征，是没有分类头的，作者提出一种利用自然语言的方法，**prompt template**。比如对于ImageNet的类别，首先把它变成"A photo of a {object}" 这样一个句子，ImageNet有1000个类，就生成1000个句子，然后这1000个句子通过之前预训练好的文本的encoder能得到1000个文本特征。直接用类别单词去抽取文本特征也可以，但是模型预训练的时候和图片配对的都是句子，推理的时候用单词效果会下降。把需要分类的图片送入图片的encoder得到特征，拿图片的特征和1000个文本特征算余弦相似性，选最相似的那个文本特征对应的句子，从而完成了分类任务。不局限于这1000个类别，任何类别都可以。彻底摆脱了categorical label的限制，训练和推理的时候都不需要提前定义好的标签列表了。

为什么刷了30个数据集？

因为是迁移学习，最终看的是泛化效果，所以在多个数据集、多个任务上进行泛化测试。

## 实验结果：

在一些普通的数据集表现好，对于专业数据集表现不好（肿瘤数据集），这种不应该做zero shot transfer应该做 few shot transfer

在各个数据集上zero shot CLIP vs resnet50：（linear probe意思是把前面学到的参数冻住，只加个分类头进行训练）

<img src="D:\markdown file\截图\image-20221228160004210.png" alt="image-20221228160004210" style="zoom:50%;" />

在20多个数据集上做zero shot CLIP vs few shot CLIP vs few shot 其他方法：（基于linear porbe）

<img src="D:\markdown file\截图\image-20221228160806764.png" alt="image-20221228160806764" style="zoom:50%;" />

在下游任务使用所有的数据：（基于 linear porbe）

<img src="D:\markdown file\截图\image-20221228162734612.png" alt="image-20221228162734612" style="zoom:50%;" />

## 创新点：

1.实际上CLIP这篇文章已经开始考虑DA问题了，只不过解决的思路是不一样的，或者不叫DA，DA局限在原有的网络架构（或者叫训练方式）上，而CLIP直接考虑使用新的网络架构（或者叫训练方式）解决DA问题。实际上这是一种更强的泛化，不仅在domain上泛化，还是先了跨任务的泛化。

<img src="D:\markdown file\截图\image-20221226215352358.png" alt="image-20221226215352358" style="zoom:50%;" />

2.CLIP简单效果好，迁移学习能力极强，预训练好的模型能在任意视觉分类的数据集上取得不错的效果，关键是zero-shot的，就是说没有在这些视觉分类的数据集上进行训练就得到了这么好的效果。

3.对于下游分类任务，可以随时增加、减少类别，不光是imagenet中的1000类，如果有新类三轮车，那么在text encoder之前加入类别文本‘三轮车’即可。然而对于仅仅在imagenet上训练的1000类分类头，对于新类三轮车是一点办法都没有的。

4.CLIP不光可以识别新类别，由于他把视觉语义和文字语义联系到一起，所以他学到的特征语义性非常强，迁移效果非常好。

5.作者其实已经证明了，预测哪个句子与哪个图像相匹配的简单预训练任务是一种有效且可扩展的方法。

6.作者证明使用大规模无标记的数据集比有监督的手工标记的数据集好使（比如GPT、BERT都使用了自监督的训练方式，所以损失与下游任务无关，模型架构与下游任务无关，他并不是在做一个特殊的分类任务，只是想通过预训练得到一个非常好的、泛化能力强的特征，所以对于下游任务，我们就不用费尽心思去研究一个针对任务的输出头，或者特殊的处理）

7.作者提出一个观点，模型效果不好可能不是模型的问题，是数据集的规模没上去，

8.作者尝试从resnet → efficient net → ViT large，作者还发现迁移学习的效果和模型的大小基本呈现正相关，是一个平滑的过程，所以可以预料到模型越大迁移效果越好。

9.**prompt engineering & ensembling**：在微调、或者推理时用的方法，而不是在预训练阶段，为什么要有这个呢？1.文本的多义性，比如只用一个词做文本的特征抽取，crane鹤、起重机，只给一个词会有歧义。2.训练时输入都是句子，测试时也应该输入句子，如果输入单个词就有domain distribution gap。对于宠物数据集可以写‘a photo of {}, a type of pet.’。那么ensembling是干什么呢，就是多用一些这种提示模板（作者用了80个模板），做多次推理，最后把结果综合起来，选次数多的。

6.互联网上面已经存在了大量的图像文本对（在网页中，开发者一般都会为图片添加一段文字备注），实际上这些素材可以作为已经标注好的数据集，所以才可能收集4E的数据，如果人工标注基本不可能实现。

数据集示例：

<img src="D:\markdown file\截图\image-20221226224801484.png" alt="image-20221226224801484" style="zoom:50%;" />

如何找呢？比如我在国外一个新闻网站找的：

新闻网站1：

<img src="D:\markdown file\截图\image-20221227205150021.png" alt="image-20221227205150021" style="zoom:33%;" />

新闻网站2：

<img src="D:\markdown file\截图\image-20221227205440945.png" alt="image-20221227205440945" style="zoom:33%;" />

7.争议：究竟是数据集的功劳还是多模态的功劳（就是把语句当作监督信息）？虽然有争议但是还是有不少工作基于CLIP进行，比如DALL-E，效果都非常好。我偏向是多模态的功劳，作者也做了实验，证明是CLIP模型就好

