## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\3.【2014】【NIPS】【Yosinski、Bengio】【康奈尔大学】【在不同的层数进行finetune和frozen看效果】How transferable are features in deep neural networks.pdf)

### 1.1 在干什么、有什么贡献

本文通过实验说明神经网络的可迁移性，说明神经网络的前几层特征是通用的，后几层的特征是争对特定任务的。

![image-20220917163439099](D:\markdown file\截图\image-20220917163439099.png)

微调是用小学习率训练，不是不训练frozen。

### 1.2 结论是什么

做出了几个贡献：

1. 定义一种方法去量化某一层的通用或特定程度。
2. 当使用转移的特征而不进行微调时，会导致性能下降的原因:(i)特征本身的特异性 (ii)由于在相邻层上的共适应神经元之间分割原网络而造成的优化困难。
3. 量化了转移特征带来的性能效益是如何随着基本任务和目标任务的不同而降低的
4. 我们将随机权值与转移权值(包括冻结权值和微调权值)进行比较，发现转移权值的性能更好
5. 用任意层的特征去初始化一个新网络，在进行微调后可以提高泛化性。

简单来说就是：

模型的迁移能力主要受到两个因素的影响：1）深度模型在越深的层，其专业性（specialization）越强，即越只能完成特定任务，这使得深度模型学习到的深层特征迁移性很差；2）模型在优化的过程中，层与层之间的参数的优化是有关联性，当固定浅层的权值，来训练高层权值时，会打破这种关联性，使得模型的能力变差，泛化能力也变差。

结论：

**结论一**：**在神经网络中层与层之间的特征的共适应性是脆弱的（*fragile co-adapted features* ）**

**结论二：影响模型迁移的两大因素，①层与层之间的共同适应下降会影响模型的可迁移性，就是后面的层（随机初始化的层）不知道前面的层（固定参数的层）提出来的特征是在干什么，②越到底层，提出的特征越来越通用，越到高层，提出的特征越来越特殊，越来越适用于某一个指定任务，文中也叫做表示的特异性（the specificity of representation）所以迁移的层数越高，就会影响模型的可迁移性。并且固定的层数少，共同适应下降是影响模型的可迁移性主要原因，固定的层数多，表示的特异性是影响模型的可迁移性主要原因。**

**结论三：使用别的数据集上（即数据集A）训练好的模型进行微调fine-tune，来训练自己数据集（即数据集B），可以有效提高模型的效果和泛化性**。

**结论四：fine-tune永远比frozen的效果好，能用fine-tune别用frozen。**

神经网络的前3层基本都是general feature，进行迁移的效果会比较好；
深度迁移网络中加入fine-tune，效果会提升比较大，可能会比原网络效果还好；
Fine-tune可以比较好地克服数据之间的差异性；
深度迁移网络要比随机初始化权重效果好；
网络层数的迁移可以加速网络的学习和优化。



## 2. 实验

### 2.1 实验名词、设置的介绍

![image-20220918113327643](D:\markdown file\截图\image-20220918113327643.png)

本文通过实验说明结果，在ImageNet的1000个类中，作者将其分成2份（A与B），每份包含500个类，这里是随机分的。然后**分别针对A和B份，分别训练一个AlexNet网络baseA和baseB**。关于AlexNet的结构就不多说了，一共8层，前面5层是卷积层，后面3层是FC层。**作者分别对n=1~7进行了实验**。

举个例子：比如n=3，那么**AnB**就是说在一个新的AlexNet上，前三层采用A网络的前三层并且将其frozen，后5层随机初始化，然后在B数据上去训练。

那么**BnB**就是用B网络的前三层frozen，剩下的5层初始化，然后在B上实验。

而**AnB+**是指使用A网络的前三层，后5层随机初始化，但是不做frozen，整个网络拿来做BP训练，也就是微调。

**BnB+**是指使用B网络的前三层，后5层随机初始化，但是不做frozen，整个网络拿来做BP训练，也就是微调。

如何证明呢？比如A3B的效果和B3B的效果差不多，那么就证明了前3层确实可以通用。如果性能下降，则有证据表明第三层特性是针对A的。

### 2.2 实验结论（这是本文重点，只看这个就行了）

![image-20220918115547180](D:\markdown file\截图\image-20220918115547180.png)

首先是**没有迁移的情况**，比如BnB网络，这就引出了**结论一**：**在神经网络中层与层之间的特征的共适应性是脆弱的（*fragile co-adapted features* ）**。什么意思呢？就是说我先在数据集B上训练一个网络baseB，然后拿出baseB前四层的网络参数固定住，后四层重新在数据集B上学习。按照道理来说，B4B网络的前四层网络是已经训练好的，这四层网络已经可以很好的提取出数据集B的特征了，然后后四层的网络再把前四层的特征拿来使用，这样应该收敛更快，效果起码不会比baseB网络差才对，但是训练出的B4B网络的效果是远差于baseB网络的。这也就说明其实后四层网络并没有合理使用前四层网络提取出的特征，或者说后四层并不知道前四层在干什么。

虽然B4B网络的效果不好，但是B6B、B7B效果又回升了，这证明固定前7层，训练最后一层也是可以的，最后一层可以利用前七层的特征完成任务。

![image-20220918154646440](D:\markdown file\截图\image-20220918154646440.png)

然后是**有迁移的情况**，比如AnB网络，在A1B、A2B网络中，性能都与baseB的性能差不多，这证明了底层的特征确实是通用的，所以对于前几层网络的参数也可以直接复制过去使用，即使没有fine-tune效果也不差。然后再来看A3B、A4B网络，其性能都有所下降，这主要是由两个因素决定，这也就是**结论二**：**影响模型迁移的两大因素，①层与层之间的共同适应下降会影响模型的可迁移性，就是后面的层（随机初始化的层）不知道前面的层（固定参数的层）提出来的特征是在干什么，②越到高层，提出的特征越来越特殊，越来越适用于某一个指定任务，文中也叫做表示的特异性（the specificity of representation）所以迁移的层数越高，就会影响模型的可迁移性**。

可以从图中看到，在A3B、A4B、A5B网络中，层与层之间的共同适应下降是影响模型效果下降的主要原因，但是这时提出的特征还是普遍的，通用的。但是在A6B、A7B网络中，提出的特征越来越特殊，越来越适用于某一个指定任务（即模型越来越适用于数据集A的任务，迁移到数据集B上效果就下降了），表示的特异性（the specificity of representation）是影响模型效果下降的主要原因。为什么这么说呢，因为如果A6B、A7B网络只受到共适应性下降的影响，其模型效果应该与B6B、B7B相同，但是事实并不是这样，虽然A6B模型效果轻微上升，但是A7B网络的效果又急剧下降了，这也可以看出表示的特异性就是A6B、A7B模型效果下降的主要原因。

![image-20220918160825164](D:\markdown file\截图\image-20220918160825164.png)

**结论三：使用别的数据集上（即数据集A）训练好的模型进行微调fine-tune，来训练自己数据集（即数据集B），可以有效提高模型的效果和泛化性**。如何解释呢？可以看作这个模型同时训练了数据集A和数据集B，即使在数据集B上训练过许多个epochs后，模型还是记得数据集A的，所以泛化性高。并且这个效果与保留多少层进行fine-tune无关，即使保留一层即A1B^+^网络，效果相比原来的baseB网络也有提升，但是保留的层越多，效果也会轻微提升。

![image-20220918162925496](D:\markdown file\截图\image-20220918162925496.png)

**结论四：fine-tune永远比frozen的效果好，能用fine-tune别用frozen。**蓝线和红线是fine-tune的结果，黑线是frozen的结果，可以看到无论n是多少，只要做fine-tune效果都比frozen很好。









