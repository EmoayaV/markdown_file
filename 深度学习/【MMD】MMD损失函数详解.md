# MMD（Maximum Mean Discrepancy）损失函数

**实际上现在基本不用这种度量的方法了，使用一个对抗过程就可以轻松拉近两个分布的距离。**

文章地址：https://zhuanlan.zhihu.com/p/163839117

## 首先介绍数学理论上的MMD

- 1.随机变量的矩是什么
- 2.如何衡量两个随机变量的差异
- 3.如何表示一个变量的任意阶矩

### 1. 怎么描述一个随机变量

去描述一个随机变量，最直接的方法就是给出它的概率分布函数 f(x) 。一些简单的分布可以这么干，比如正太分布给出均值和方差就可以确定，但是对于一些复杂的、高维的随机变量，我们无法给出它们的分布函数。

这时候我们可以用随机变量的矩来描述一个随机变量，比如一阶中心矩是均值，二阶中心矩是方差等等。如果两个分布的均值和方差都相同的话，它们应该很相似，比如同样均值和方差的高斯分布和拉普拉斯分布。但是很明显，均值和方差并不能完全代表一个分布，**这时候我们就需要更高阶的矩来描述一个分布**。

举个例子，就好比描述人一样，如果两个人身高、轮廓都一样，我们会说这两个人很像。但是如果要说这两个人是一个人的话，我们如要更多的信息，比如血型、DNA等更加复杂的信息

而**MMD的基本思想就是，如果两个随机变量的任意阶都相同的话，那么两个分布就是一致的**。而**当两个分布不相同的话，那么使得两个分布之间差距最大的那个矩应该被用来作为度量两个分布的标准。**

### 2. 如何衡量两个随机变量的差异

MMD常被用来度量两个分布之间的距离，是迁移学习中常用的损失函数。定义如下，x 的分布为 p ，y 的分布为 q：

<img src="D:\markdown file\截图\image-20221008121944225.png" alt="image-20221008121944225" style="zoom: 50%;" />

这个公式里面有四个符号，第一个是 sup 求上界（简单理解就是求最大值），第二个是 E~p~ 表示求期望，第三个是 f(⋅) 表示映射函数，第四个H表示函数 f 在再生希尔伯特空间中的范数应该小于等于1。

### 3. 如何表示一个随机变量的任意阶矩

刚才讲到，两个分布应该是由高阶矩来描述的，那么如何获得一个随机变量的高阶矩呢？**核函数。**

在支持向量机中我们都知道有一个高斯核函数，它对应的映射函数恰好可以映射到无穷维上，映射到无穷维上再求期望，正好可以得到随机变量的高阶矩，这个方法有一个更高大上的名字，叫做kernel embedding of distributions，这个简单理解就是将一个分布映射到再生希尔伯特空间（每个核函数都对应一个RKHS）上的一个点。

**这样两个分布之间的距离就可以用两个点的内积进行表示!**

### 4. MMD的介绍

MMD（最大均值差异）是迁移学习，尤其是Domain adaptation （域适应）中使用最广泛（目前2018）的一种损失函数，主要用来度量两个不同但相关的分布的距离。两个分布的距离定义为：

![image-20221008123016415](D:\markdown file\截图\image-20221008123016415.png)

其中 H 表示这个距离是由 ϕ ( ) 将数据映射到再生希尔伯特空间（RKHS）中进行度量的。
具体推导见：https://blog.csdn.net/a529975125/article/details/81176029

## 论文实际中的MMD

<img src="D:\markdown file\截图\image-20221007135154819.png" alt="image-20221007135154819" style="zoom:50%;" />

这里我给出简单的公式释义：就是源域中样本（共 N~s~ 个）通过CNN后，在第 m 层的输出特征记做 f^(m)^(x~si~) 。为了方便叙述，我们将f^(m)^(x~si~) 记做**源域特征**。同理目标域中样本（共 N~t~ 个）通过CNN的第 m 层的特征记做 f^(m)^(x~ti~)，记做**目标域特征**。

上面公式的意思就是先将**源域样本**与**目标域样本**映射到**重构核希尔伯特空间(RKHS)**，**这个RKHS实际上就是神经网络第m层的特征空间**，然后分别计算其在该空间的均值，最后计算均值间的**欧式距离**，即为MMD。

<img src="D:\markdown file\截图\image-20221008124044590.png" alt="image-20221008124044590" style="zoom:50%;" />

首先看看L2范数的定义：L2范数,也叫欧几里得范数:是一个向量中所有元素取平方和，然后再开平方。

<img src="D:\markdown file\截图\image-20221008124504380.png" alt="image-20221008124504380" style="zoom:50%;" />

那么L2范数的平方 |·|~2~^2^ 是什么呢？就是一个向量中所有元素取平方和。

<img src="D:\markdown file\截图\image-20221007135154819.png" alt="image-20221007135154819" style="zoom:50%;" />

**这时再来看看MMD就不难理解了反而变得十分简单**，就是输入N~t~个样本，最后一层得到N~t~个特征，把目标域N~t~个特征的和求平均，减去源域N~s~个特征的和求平均，减的结果就是一个d维的特征，之后再把得到的d维特征求所有元素取平方和，这就是MMD。