## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\26.【2016】【TPAMI】【Artem Rozantsev】【不使用权重共享，使用权重相关】Beyond Sharing Weights for Deep Domain Adaptation.pdf)

### 1.1 在干什么、有什么贡献、什么结论

之前的DA方法，都是使用对称的结构和共享的权重，将源域和目标域的数据都输入进网络进行训练，他们假设通过这样的方法，可以得到域不变的特征，在这个假设下采用权重共享和对称结构是合理的。

但是这样的假设实际上是难以实现的，作者也证明了强行假设可以学到域不变的特征，是不利于性能的提升的。所以接着作者引入一个损失函数，当它们互为线性变换时，损失函数是最小的。引入一个准则来自动确定哪些层应该共享它们的权值，哪些层不应该共享。总的来说就是**两个网络权值共享这样的作法太暴力了，作者换了一种方法，就是使两个网络权重是相关的，但不是完全独立的，可以看作使弱化版的权值共享。**为了使权重相关，作者提出了步共享权重的结构：**双流结构(two-stream architecture)**。 一个流在源域上操作，另一个在目标域上操作，虽然我们允许两个流之间对应层的权重不同，但我们也要防止它们彼此相差太远。

## 2. 网络结构

DDC：

<img src="D:\markdown file\截图\image-20221109150209704.png" alt="image-20221109150209704" style="zoom:50%;" />

作者的网络：

<img src="D:\markdown file\截图\image-20221111175752940.png" alt="image-20221111175752940" style="zoom:50%;" />

作者的网络和DDC网络其实差不多，最大的区别就是作者将中间层共享的权重参数，换为了相关的权重参数，主要是在每一层通过引入损失函数来实现。

图中绿色部分的损失就是DDC中的源域类别损失，主要是能正确分类，紫色部分的损失就是DDC中的MMD损失，主要是拉近源域目标域的分布，棕色部分的损失是作者自己加的，主要是拉近两个网络每一层的参数，以此替代权值共享。

**损失函数：**

<img src="D:\markdown file\截图\image-20221111180354264.png" alt="image-20221111180354264" style="zoom: 33%;" />

<img src="D:\markdown file\截图\image-20221111180425821.png" alt="image-20221111180425821" style="zoom: 33%;" />

公式(1)是总损失函数，公式(2)(3)是分类损失函数(如果目标域有标签则有公式(3))，公式(4)(6)(7)是每一层之间的损失，主要是让两个网络每一层的参数都相关但不相等。可以看到作者是用L2范数来拉近两个网络参数的相似度。

## ==启发：==

~~==这篇文章很有启发，他这每个层之间用的是MMD损失，那我可不可以在每个层之间用对抗损失？但是这个方法最大的问题就是太麻烦了，如果用VIT提取特征，那这个方法还可行吗？==~~

==其次这篇文章的这个想法其实就是无监督的fine-tune，不知道后续还有没有工作==

==作者还说用合成的图片放入网络效果更好，可不可以加入这篇文章[【2017】【CVPR】【Konstantinos Bousmalis】【谷歌】【训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容】Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks](C:\Users\Emoaya\Desktop\文章\泛读文章\【2017】【CVPR】【Konstantinos Bousmalis】【谷歌】【训练一个模型来改变来自源域的图像，使其看起来像是来自目标域的采样，同时保持其原始内容】Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks.pdf)的内容？==