![image-20220913130019032](D:\markdown file\截图\image-20220913130019032.png)

为什么Transformer的效果好呢？因为引入的归纳偏置，或者叫先验假设比较少，就是引入人为的经验较少，仅仅引入了位置编码，所有的东西都是由机器自己学，所以模型上限高，但是缺点是对数据量的需求大。

先验假设相当于一个提高下限降低上限的操作，如果引入先验假设，就是人为的经验，那么模型可能在几万个样本上就训练的很好，比如CNN,RNN,LSTM等等，但是随着样本数量的上升，先验假设也成为了限制模型效果的一个束缚，因为模型只能按照人为设定好的先验假设学习，没有自己去学一种先验假设来提高效果。

如果没有引入先验假设，那么就相当于一个提高上限降低下限的一个操作，因为引入的先验假设少了，模型就必须自己去学习一种先验假设，这个学习的过程需要大量的数据作为支撑，虽然上限提高了，但是如果在数据很少的情况下，训练效果是不如引入先验假设的模型的，随着数据量的上升，少先验假设的模型也会超过多先验假设的模型，比如Transformer就单单引入了一个位置编码这个先验假设，所以Transformer想训练的好必须要有百万级别的数据量，或者直接用别人训练好的模型。

ViT只使用了Transformer的Encoder部分。

![image-20220913125339960](D:\markdown file\截图\image-20220913125339960.png)

首先把Transformer用到图像上第一个难点就是输入单位是什么，对于NLP的任务，输入的可以是单个字母，也可以是单个词，通常一句话最多就几十个输入就够了，但是对于图像来说，如果以像素为单位，一张图片最少都有几百个像素，如果以像素为单位输入，这对于Transformer来说是不可以接受的。其次，如果以像素为单位输入，那么一个像素点的信息是很少的，不像NLP任务中一个字包含的信息量那么丰富，对于图片来说信息量主要是在一块区域，所以最直接的想法是把图像分成很多块，当作输入送入Transformer。



![image-20220913132329820](D:\markdown file\截图\image-20220913132329820.png)









