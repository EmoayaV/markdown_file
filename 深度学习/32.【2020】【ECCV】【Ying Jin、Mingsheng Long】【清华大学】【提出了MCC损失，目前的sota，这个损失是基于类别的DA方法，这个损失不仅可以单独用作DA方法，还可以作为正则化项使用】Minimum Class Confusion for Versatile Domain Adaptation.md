## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\32.【2020】【ECCV】【Ying Jin、Mingsheng Long】【清华大学】【提出了MCC损失，目前的sota，这个损失是基于类别的DA方法，这个损失不仅可以单独用作DA方法，还可以作为正则化项使用】Minimum Class Confusion for Versatile Domain Adaptation.pdf)

[文章解读1](https://zhuanlan.zhihu.com/p/420761545)

[文章解读2](https://blog.csdn.net/qq_45802280/article/details/127520690)

### 1.1 在干什么、有什么贡献、什么结论

<img src="D:\markdown file\截图\image-20221118185903806.png" alt="image-20221118185903806" style="zoom:50%;" />

**目前的SOTA**

本文研究了**通用域适应(Versatile Domain Adaptation VDA)**，即一种方法可以处理多个不同的域适应场景而无需进行任何修改。为了实现这一目标，应该探索一种更普遍的**归纳偏置(inductive bias)**，而不是域对齐。我们将深入研究现有方法中缺失的一块：**类混淆(class confusion)**。

为此，我们提出了一个损失函数：**最小类混淆(Minimum Class Confusion MCC)**。

该方法具有以下特点：(1)非对抗性DA方法，无需域对齐，收敛速度较快。(2)一种通用(Versatile)的方法，可以处理四种现有的场景:封闭集、部分集、多源和多目标DA，在这些场景中优于最先进的方法。此外，它还可以作为一种通用正则化器(general regularizer)，与各种现有的DA方法正交互补，加速收敛，并将这些方法推向更强。

<img src="D:\markdown file\截图\image-20221118192841105.png" alt="image-20221118192841105" style="zoom:50%;" />

我们深入研究了目标域的误差矩阵，发现在源域上训练的分类器可能会在区分正确的类和类似的类(如汽车和卡车)时产生混淆。这些发现为VDA提供了一个新的视角:类混淆(class confusion)，分类器会混淆正确类和模糊类之间的预测，**我们发现对于所有的域适应情况，减少类混淆会导致更多的转移增益。**

**这里简单说一下个人的理解：常规的UDA方法都在feature space空间上做文章，但是对于不同的上述问题而言，label space是不同的，不能直接套用。本文的思路就是从class confusion出发，因为类别间的混淆程度无论是在源域还是目标域都是一致的。**

**作者的动机就是减少类混淆，比如减少汽车与卡车类别的混淆，会增加迁移学习的效果，就是DA的效果。**

## 2. 具体细节

<img src="D:\markdown file\截图\image-20230103171205230.png" alt="image-20230103171205230" style="zoom:50%;" />

由于DNN网络的预测结果熵值过大(做出过于自信的预测)，所以先把类别的预测结果改为软标签的形式：

<img src="D:\markdown file\截图\image-20221120133121157.png" alt="image-20221120133121157" style="zoom:67%;" />

这样可以包含更多的信息量。

计算两个类别(类别 j 和类别 j‘ )预测值的相关性：

<img src="D:\markdown file\截图\image-20221120133527338.png" alt="image-20221120133527338" style="zoom:67%;" />

当预测接近于均匀分布，没有显示明显的峰值(对某些类来说明显更大的概率)时，我们认为分类器对这个示例一无所知。当预测显示出几个峰值时，表示分类器在几个模糊的类(如car和truck)之间不太愿意。

进一步，我们引入了一种基于不确定性的权重机制，以便更准确地量化类混淆。具体来说，我们使用信息熵作为分布不确定性度量：

<img src="D:\markdown file\截图\image-20221120135802026.png" alt="image-20221120135802026" style="zoom:67%;" />

虽然熵是对不确定性的一种度量，但我们想要的是一个概率分布，它将更大的概率放在具有更大确定性的类预测实例上。具体来说，用softmax的形式将信息熵转换为概率值：

<img src="D:\markdown file\截图\image-20221120135941735.png" alt="image-20221120135941735" style="zoom:67%;" />

类混淆的定义：

<img src="D:\markdown file\截图\image-20221120140214321.png" alt="image-20221120140214321" style="zoom:67%;" />

类正则化：

<img src="D:\markdown file\截图\image-20221120140353699.png" alt="image-20221120140353699" style="zoom:67%;" />

总之，最后的最小化类混淆损失为：

<img src="D:\markdown file\截图\image-20221120140159728.png" alt="image-20221120140159728" style="zoom:67%;" />

总损失为：

<img src="D:\markdown file\截图\image-20221120141149236.png" alt="image-20221120141149236" style="zoom:67%;" />

不仅如此，还可以当作现有DA方法的正则化：

<img src="D:\markdown file\截图\image-20221120141625897.png" alt="image-20221120141625897" style="zoom:67%;" />

## 创新点

==这是一类新方法的开篇之作，这个MCC损失不仅可以当作损失用，还可以当作正则化项用==

作者提到的最小化类混淆损失可以用于所有的DA任务中，包括OSDA、PDA、UDA、UniDA等，都有不错的性能提升，不用知道具体的细节，只用知道这个loss很有用就行了。

**之前文章关注于特征**，比如coral和DANN等方法都是拉近特征分布的距离，得到域不变的特征，之前提到的概念是**域混淆**，**这篇文章关注于分类预测**，这里主要提到的是**类混淆**。

<img src="D:\markdown file\截图\image-20221120142130205.png" alt="image-20221120142130205" style="zoom:50%;" />

==以前的DA文章方向基本是拉近特征分布，这篇文章直接拉近类别的分布。与主流不符，选择主流是无监督，就是没有类别标签的方法。==
