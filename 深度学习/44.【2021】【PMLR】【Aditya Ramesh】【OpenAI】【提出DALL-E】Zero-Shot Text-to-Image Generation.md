## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\44.【2021】【PMLR】【Aditya Ramesh】【OpenAI】【提出DALL-E】Zero-Shot Text-to-Image Generation.pdf)

[文章解读1](https://www.zhihu.com/question/447757686/answer/2326092032)

[文章解读2](https://zhuanlan.zhihu.com/p/394467135)

### 1.1 在干什么、有什么贡献、什么结论

文章写的非常差，直接看文章解读。

背景：生成模型，输入字输出图片

问题：文本到图像的生成

解决：基于Transformer、dVAE、BPE，使用三阶段发方法，进行文本到图像的生成，**提出DALL-E**

## 2. 网络结构

<img src="D:\markdown file\截图\image-20221223194025260.png" alt="image-20221223194025260" style="zoom:50%;" />

在第一个阶段，将256×256的图片分为32×32个patch，然后使用训练好的[离散VAE模型](https://www.zhihu.com/search?q=离散VAE模型&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})的encoder将每个patch映射到大小为8192的词表中，最终一张图片转为用1024个[token](https://www.zhihu.com/search?q=token&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})表示。

在第二个阶段，使用BPE-encoder对文本进行编码，得到最多256个token，token数不满256的话padding到256；再将256个文本token与1024个图像token进行拼接，得到长度为1280的数据；最终将拼接的数据输入训练好的具有120亿参数的[Transformer模型](https://www.zhihu.com/search?q=Transformer模型&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})。

在第三个阶段，对模型生成的图像进行采样，并使用同期发布的CLIP模型**[2]**对采样结果进行排序，从而得到与文本最匹配的生成图像。生成多个图片分别对一句话用CLIP继续排序，挑出最好的。

DALLE包括三个独立训练得到的模型：dVAE，Transformer和CLIP，其中dVAE的训练与VAE基本相同，Transformer采用类似GPT-3的生成式预训练方法。下面对DALL-E采用的dVAE模型和Transformer模型做简单介绍，对CLIP感兴趣的朋友可以参考**[2]**

- **dVAE**

dVAE主要用来为图像的每个[patch](https://www.zhihu.com/search?q=patch&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})生成token表示，这次openAI开出的代码就是dVAE的[推理代码](https://www.zhihu.com/search?q=推理代码&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})。dVAE的encoder和decoder的机构较为简单，都是由bottleneck-style的resblock组成，但与常见的VAE相比，dVAE有以下两点区别：

1、dVAE的encoder是将图像的patch映射到8192的词表中，论文中将其分布设为

在词表向量上的均匀分类分布，这是一个[离散分布](https://www.zhihu.com/search?q=离散分布&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})，由于不可导的问题，此时不能采用重参数技巧。DALL-E使用了Gumbel-SoftMax trick来解决这个问题，对Gumbel-SoftMax trick感兴趣的朋友可以参考**[3]**。

2、在重建图像时，真实的像素值是在一个[有界区间](https://www.zhihu.com/search?q=有界区间&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})内，而VAE中使用的Gaussian

分布和Laplace分布都是在整个实数集上，这造成了不匹配的问题。为了解决这个问题，论文中提出了logit-Laplace分布，如下式所示：

<img src="D:\markdown file\截图\image-20221223195216284.png" alt="image-20221223195216284" style="zoom:50%;" />



- **Transformer**

Dall-E中的Transformer结构由64层attention层组成，每层的注意力头数为62，每个注意力头的维度为64，因此，每个token的向量表示维度为3968。如图2所示，attention层使用了行注意力mask、列注意力mask和卷积注意力mask三种稀疏注意力。

<img src="D:\markdown file\截图\image-20221223195251529.png" alt="image-20221223195251529" style="zoom:50%;" />

Transformer的输入如图3所示，其中pad embd通过学习得到，根据论文介绍，为每个位置都训练了一个pad embd，即256个pad embd，在对文本token进行pad时，使用对应位置的pad embd。

<img src="D:\markdown file\截图\image-20221223195305553.png" alt="image-20221223195305553" style="zoom:50%;" />

总的来说，目前公开的DALL-E的实现在模型结构上并没有太多创新，而是合理利用了现有的模型结构进行组合，并采用了一些trick解决了遇到的问题，从而在[大数据集](https://www.zhihu.com/search?q=大数据集&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})上训练得到超大规模的模型，取得了令人惊艳的效果，这也符合openAI的一贯风格。但无论如何，DALL-E在深度学习能力边界探索的道路上又前进了一步，也再一次展示了大数据和超大规模模型的魅力。美中不足的是，DALL-E包含了三个模块，更像是一个[pipeline](https://www.zhihu.com/search?q=pipeline&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1764970196})，而对于普通的研究者来说，要运行这样一个复杂的大规模模型是一件很困难的事情。