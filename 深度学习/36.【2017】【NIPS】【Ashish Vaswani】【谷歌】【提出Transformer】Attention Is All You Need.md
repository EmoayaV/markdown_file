## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\36.【2017】【NIPS】【Ashish Vaswani】【谷歌】【提出Transformer】Attention Is All You Need.pdf)

### 1.1 在干什么、有什么贡献、什么结论

提出Transformer。

## 2. 网络结构

<img src="D:\markdown file\截图\image-20221123164527968.png" alt="image-20221123164527968" style="zoom: 67%;" />

首先网络大体分为两个部分，分别是Encoder和Decoder部分。

**Encoder部分：**

首先Encoder部分是N个block的堆积(stack)，这个block中有2个子层(sub-layer)，蓝色部分就是1个全连接神经网络，橙色部分是1个多头的注意力机制，在这两个层中间加入了残差链接(residual connection)，之后又接了一个层归一化(layer normalization)，如黄色部分所示。

**Decoder部分：**

Decoder部分也是N个block的堆积，这个block中有3个子层(sub-layer)，蓝色部分就是1个全连接神经网络，橙色部分是1个多头的注意力机制，1个masked的多头注意力，这个masked主要是为了保证因果性，在这两个层中间加入了残差链接(residual connection)，之后又接了一个层归一化(layer normalization)，如黄色部分所示。

**注意力机制：**

<img src="D:\markdown file\截图\image-20221123175505428.png" alt="image-20221123175505428" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221123174703942.png" alt="image-20221123174703942" style="zoom:50%;" />

这里作者用的是点积注意力机制(Scaled Dot-Product Attention)，上图是这种注意力机制的矩阵描述，其中最上面得到QKV矩阵中的WI，这个I矩阵在transformer是input embedding + position embedding，其余部分基本一致。多头的注意力机制，实际上就是把输入向量分成两份，在做注意力机制。

<img src="D:\markdown file\截图\image-20221123175917541.png" alt="image-20221123175917541" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221123180308996.png" alt="image-20221123180308996" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221123175119726.png" alt="image-20221123175119726" style="zoom:50%;" />   

<img src="D:\markdown file\截图\image-20221123181114776.png" alt="image-20221123181114776" style="zoom:50%;" /><img src="D:\markdown file\截图\image-20221123181208963.png" alt="image-20221123181208963" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221123181253374.png" alt="image-20221123181253374" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221123181401801.png" alt="image-20221123181401801" style="zoom:50%;" />

![image-20230519153053299](D:\markdown file\截图\image-20230519153053299.png)

![image-20230519153108724](D:\markdown file\截图\image-20230519153108724.png)

![image-20230519153139334](D:\markdown file\截图\image-20230519153139334.png)

![image-20230519153200428](D:\markdown file\截图\image-20230519153200428.png)

![image-20230519153323085](D:\markdown file\截图\image-20230519153323085.png)



多头注意力机制：首先用矩阵乘法将输入的input embedding + position embedding变成QKV，接着计算出QK的乘积，接着进行scale操作，这里乘的1/√d 的d是Q的维度(或者也可以理解为词向量的长度，这个视频里是4)，最后逐元素的进行softmax操作，得到一个权重矩阵，最后把这个权重矩阵和V相乘就得到了输出。由于是多头注意力机制，所以最后要进行concat操作，再经过一个全连接层就得到了最终的输出。

Add & Norm：得到输出后还要进行层归一化、加和操作。

Feed Forward：就是经过一个全连接神经网络。