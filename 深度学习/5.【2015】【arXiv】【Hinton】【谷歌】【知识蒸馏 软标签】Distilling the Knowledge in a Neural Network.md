## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\5.【2015】【arXiv】【Hinton】【谷歌】【知识蒸馏 软标签】Distilling the Knowledge in a Neural Network.pdf)

### 1.1 在干什么、有什么贡献

![image-20220920172156516](D:\markdown file\截图\image-20220920172156516.png)

![image-20220921170624340](D:\markdown file\截图\image-20220921170624340.png)

![image-20220921170809522](D:\markdown file\截图\image-20220921170809522.png)

一种简单的提高机器学习算法性能的方法是**集成学习**，就是在相同的数据上训练许多不同的模型，测试时计算它们的prediction的average。**集成学习**的缺点是模型笨重，计算成本高，难以部署给大量用户，特别是某个模型太大时。**本文提出一种压缩技术来将集成模型中的知识distilling（蒸馏）到单个模型中。**

**知识蒸馏（Knowledge Distilling）是模型压缩的一种方法**，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。

**这是知识蒸馏的开山之作**，辛顿于2015年发表的一篇论文提出！主要原因是之前提出的各种模型，无论是VGG16,Resnet，还是其他一些模型，它们都对实时性没有要求，而且模型都很大，计算量大，所以这样的模型是很难在资源有限的小型移动设备上应用的，所以辛顿提出了知识蒸馏的思想，就是用比较大的网络作为教师网络（teacher network），训练出soft target，这种soft target会表示出分类的相对信息，通俗的讲就是让机器去识别一辆宝马车，硬分类就是是宝马车为1，不是为0.而软分类是不仅知道是宝马车的概率，还知道它不是土豆的概率，不是拖拉机的概率，很明显，不是土豆和不是拖拉机的概率差别是很大的。所以，我们用软分类知道了非常重要的相对信息，这些信息会帮我们大大减少数据量。
**主要工作：**

- 提出一种知识蒸馏的方法，可以压缩模型，让小模型达到与集成亦或是大型模型相似的性能
- 提出一种新的集成学习方法，可以让模型训练速度更快，并且是并行训练



## 2. 一些细节、动机

### 2.1 动机

**动机一：**Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。

**动机二：**大型模型往往不适合线上部署，一方面是计算资源消耗大，另一方面是响应速度慢，因此Hinton便考虑是否可以将大模型的知识迁移到小模型上，这里就产生了**两个问题**：

### 2.2 **大型模型知识迁移到小型模型后，小型模型应该具有什么样的表现？**

最终目的是让小型模型的泛化能力和大型模型一致。

### 2.3 **什么是知识呢？**

**先说结论，知识就是软标签soft-target。为什么这么说呢？因为他不仅可以用最大的概率判断类别，还可以使用其他的概率判断这图片与其他类别的相似度是多少。其他类别的预测概率信息也是是否重要的。**

将知识看成是模型参数不合适，这太过复杂且难以入手，结合上一个疑问，个人认为，可以把模型知识抽象为泛化能力，如何让小型模型学得大型模型的泛化能力呢？在此之前，先了解一下什么是soft target。

对于分类模型而言，模型的输出是softmax，例如一个猫狗猪的三分类，对于一张狗的图片，模型输出为
[ 0.1 , 0.89 , 0.01 ] [0.1,0.89,0.01]
[0.1,0.89,0.01]

那说明大型模型认为这张图片属于狗，但具有一些猫的分类特征，大型模型的输出含有两方面信息，一是这张图片属于什么类，二是与这张图片相似的类是什么，**大型模型的输出又被称为是soft target(也有些变化，计算公式在下面)，hard target即为one hot编码，相比于soft target，hard target的信息较少，只能表明一张图片是什么类别。**

回到原先的问题，**如果让小型模型去拟合大型模型的soft target，以期让小型模型学会像大型模型一样思考，那么小型模型的泛化能力不就可能和大型模型一致了吗？这便是论文的出发点。**

### 2.4 为什么蒸馏有用？

**好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据。**所以蒸馏的目标是让student学习到teacher的泛化能力，理论上得到的结果会比单纯拟合训练数据的student要好。另外，对于分类任务，如果soft targets的熵比hard targets高，那显然student会学习到更多的信息。

## 3. 方法

### 3.1 为什么要用软标签？

作者指出，粗暴地使用one-hot编码丢失了类间和类内关于相似性的额外信息。举个例子，在手写数字识别时，22和33就长得很像。但是使用上述方法，完全没有考虑到这种相似性。对于已经训练好的模型，当识别数字2时，很有可能它给出的概率是：数字2为0.99，数字3为 10^-2^ ，数字7为 10^-4^ 。

### 3.2 具体实现

更具体一点，论文将softmax的输出更改为：

![image-20220920165827676](D:\markdown file\截图\image-20220920165827676.png)

大型模型与小型模型的输出均采用上式计算，采用上式计算的大模型输出即为soft target。

对Teacher网络的输入如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。

**T为超参数，T越大，可以产生更加soft的target**

为什么要引入一个超参数呢？首先不论是否引入T，小型模型都是拟合大型模型的输出，简单举个例子，假设大型模型的输出为

小型模型的输出为
[ P~1~( x ) , P~2~( x ) , P~3~( x ) ] 

则交叉熵损失函数为
− [ 0.1 log ⁡P~1~( x ) + 0.89 log P~2~( x ) + 0.01 log ⁡P~3~( x )]

第二项对于损失函数的取值影响很大，如果小型模型的输出为[0.3，0.67，0.03]，那么交叉熵损失函数值大致为0.7，如果设置T为2，交叉熵损失函数值大致为1.62，比前者大很多，而小型模型的输出离大型模型还是有差距的，所以设置一个超参数T，可以让小型模型更好的拟合大型模型的输出。

换个角度来看，hard target只能表明图片是什么，而soft target还可表明与该图片相似的类别是什么，knowledge distillation相当于一种监督信息的弥补。

### 3.3 损失函数

有两种蒸馏的目标函数：

**只使用soft targets**：在蒸馏时teacher使用新的softmax产生soft targets；student使用新的softmax在transfer set上学习，和teacher使用相同的T。

**同时使用sotf和hard targets：**学生student的目标函数是hard target和soft target目标函数的加权平均，使用hard target时T=1，soft target时T和teacher的一样。Hinton的经验是给hard target的权重小一点。

另外要注意的是，因为在求梯度（导数）时新的目标函数会导致梯度是以前的 1/T2 ，所以要再乘上 T2 ，不然T变了的话hard target不减小（T=1），但soft target会变。

上面这话有些歧义，实际上是在soft target的Loss项乘上T^2，因为Loss(soft target)和Loss(hard target)对logits的导数，前者是后者的1/T^2，只有对soft target的Loss项乘以T^2，两者才是一个数量级，才能保证反向求导时两者的权重基本相同。

采用soft target的交叉熵损失函数为L^soft^（小型模型的输出也要除以T后计算softmax），采用hard target的交叉熵损失函数为L^hard^，单纯使用soft target的确可以使用不需要标记的数据训练小型模型，但是Hinton发现联合使用两个损失函数效果更佳，如下
L = α L^soft^ + (1 − α) L^hard^

通常α 的取值较大，具体流程如下图所示:

![image-20220920172156516](D:\markdown file\截图\image-20220920172156516.png)

### 3.4 一些经验和小trick

- 实验证实，Soft target可以起到正则化的作用（不用soft target的时候需要early stopping，用soft target后稳定收敛）
- 数据过少的话无法完整表达teacher学到的知识，需要增加无监督数据（用teacher的预测作为标签）或进行数据增强，可以使用的方法有：1.增加[MASK]，2.用相同POS标签的词替换，2.随机n-gram采样，具体步骤参考文献2

- T越大越能学到teacher模型的泛化信息。比如MNIST在对2的手写图片分类时，可能给2分配0.9的置信度，3是1e-6，7是1e-9，从这个分布可以看出2和3有一定的相似度，因此这种时候可以调大T，让概率分布更平滑，展示teacher更多的泛化能力
- T可以尝试1～20之间



## 4. 实验

![image-20220921180612774](D:\markdown file\截图\image-20220921180612774.png)

主要意思就是说，蒸馏或者叫加入软间隔确实可以将在小模型上达到大模型的效果，还有一个有意思的现象，就是对于没有出现在小模型的训练数据中的数据，如果使用蒸馏，还是可以在小模型上完成没见过的数据的分类任务。比如说在小模型的数据集中去除数字3，使用蒸馏后还可以识别出3.

![image-20220921181858839](D:\markdown file\截图\image-20220921181858839-1663755539612-1.png)

第一行baseline是单个大模型的效果，第二行是10个集成模型的效果的平均，第三行是使用蒸馏学习的效果。

![image-20220920180908139](D:\markdown file\截图\image-20220920180908139.png)

仅使用了3%的训练数据，便能得到与使用100%训练数据的模型的效果，这说明soft target的监督能力足够强劲

## 5. 一点补充

### 5.1 知识蒸馏的位置

![image-20220921140922943](D:\markdown file\截图\image-20220921140922943.png)

知识蒸馏主要是轻量化神经网络中的一个技术，如何轻量化一个神经网络呢？由四种方式，分别是**压缩已经训练好的模型，直接训练轻量化的网络，加速卷积运算，硬件部署**，知识蒸馏就是压缩已经训练好的模型中的一种技术。

我最多研究前两种轻量化神经网络的方式，后两种太底层了无法研究。

### 5.2 知识蒸馏的代码包

![image-20220921150107710](D:\markdown file\截图\image-20220921150107710.png)

![image-20220921150130406](D:\markdown file\截图\image-20220921150130406.png)

### 5.3 具体的训练过程的图示

![image-20220921172026562](D:\markdown file\截图\image-20220921172026562.png)

这里注意一下，温度T越大类别之间的概率越平滑，信息熵越大，比如 T = 1，label = [0.1, 0.85, 0.05]，T = 10，label = [0.3, 0.5, 0.2]，类似于这样。

![image-20220921173729398](D:\markdown file\截图\image-20220921173729398.png)
