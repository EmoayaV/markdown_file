## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\23.【2020】【NIPS】【Kuniaki Saito】【波士顿大学、MIT】【UDA，DANCE，引入熵分离损失和邻域聚类损失】Universal Domain Adaptation through Self-Supervision.pdf)

[文章解读](https://zhuanlan.zhihu.com/p/421407725)

### 1.1 在干什么、有什么贡献、什么结论

**背景：目标域无监督。**

提出了**新的邻域聚类技术**，**Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE).**

**要解决的问题：**1.现有的工作严重依赖于关于类别转换的先验知识。即需要预先知道是闭集的情况还是开集的情况。2. 过度依赖来源监督使得在目标上获得有区别的特征具有挑战性。

DANCE不是仅仅依靠对源类别的监督来学习区别性表示，对于目标域数据，通过一种“**邻域聚类neighborhood clustering(NC)**技术，在目标域中使用自我监督来聚类目标样本，每个目标点要么与源中的“已知”类原型对齐，要么与目标中的相邻类对齐，这是通过最小化点相似度分布的熵来实现的。对于源域数据，我们除了进行特征对齐，还引入一种新的**熵分离损失entropy separation(ES) loss**。这种损失允许模型或者将每个目标域样本与源域样本匹配，或者将其作为“未知”类别拒绝。

我们采用一个**基于中心的分类器prototype-based classifier**，将样本映射到它们真正的类中心附近，不光对已知类，也对未知类。

我们还用到了**基于域的BN domain-specific batch normalization**。

**实际上他这个可以看作是对比学习了，因为构造了W（权重矩阵），类似于字典**

## 2. 网络结构图

作者的网络图是基于[【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】Semi-supervised Domain Adaptation via Minimax Entropy](C:\Users\Emoaya\Desktop\文章\泛读文章\【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】Semi-supervised Domain Adaptation via Minimax Entropy.pdf)这篇文章的。

<img src="D:\markdown file\截图\image-20221107134734372.png" alt="image-20221107134734372" style="zoom:50%;" />

下面是具体的方法图：

<img src="D:\markdown file\截图\image-20221106173019347.png" alt="image-20221106173019347" style="zoom:50%;" />

## 3. 邻域聚类neighborhood clustering

**符号：**目标域有N~t~个无标签样本，G是特征提取器输入样本x输出特征f，单个特征f的维度是[1, d]，W=[w1, w2, ..., wK]是分类网络（单个线性层）的权重参数，W的维度是[d, K]，K是源域类别的个数，p表示f经过W后再经过softmax层得到的概率。

**中心思想：**我们不依赖与源的严格分布对齐，以提取鉴别的目标特征，所以我们建议最小化每个目标样本特征与其他目标样本的相似度分布的熵，最小化每个目标样本特征与源域类别中心(source prototypes)的相似度分布的熵。为了使熵最小化，目标样本特征将向附近的点(假设存在邻居)或源域类别中心靠近。

**公式：**

==V表示存放所有目标域样本特征的储存器，V的维度是[d, N~t~]，==F表示所有目标域样本特征和源域中心权重向量(prototype wright vector)，F的维度是[d, N~t~+K]，F~j~表示F中第j个向量，fi表示在一个小批量中的目标域样本的特征。

<img src="D:\markdown file\截图\image-20221107152547880.png" alt="image-20221107152547880" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221107152919535.png" alt="image-20221107152919535" style="zoom:50%;" />

P~ij~表示输入特征为fi的条件下，fi是邻居特征还是类别中心的概率，其中τ控制着分布的集中度(在知识蒸馏里叫做温度)，所以也可以控制聚类的个数。

<img src="D:\markdown file\截图\image-20221107153202246.png" alt="image-20221107153202246" style="zoom:50%;" />

==对于F = [V~1~, V~2~, ..., V~Nt~]，F~j~ · f~i~ 可以理解为输入特征f~i~ 与其他目标域的特征计算相似度。对于F = [w1, w2, ..., wK]，F~j~ · f~i~ 可以理解为输入特征f~i~ 经过源域分类器计算属于源域类别的概率。==

==这里的点积其实可以等价于余弦相似度？==

<img src="D:\markdown file\截图\image-20221107153935735.png" alt="image-20221107153935735" style="zoom:50%;" />

**损失的计算：**

注意这里是信息熵不是交叉熵。

<img src="D:\markdown file\截图\image-20221107153706035.png" alt="image-20221107153706035" style="zoom:50%;" />

**具体流程：**

<img src="D:\markdown file\截图\image-20221107151935199.png" alt="image-20221107151935199" style="zoom: 50%;" />

<img src="D:\markdown file\截图\image-20221107152350842.png" alt="image-20221107152350842" style="zoom:50%;" />

==对于一个目标域小批量样本中的一个样本，我们先得到这个样本的特征，比如f1， 然后再去计算概率pij，这个概率既是目标域样本特征与其他所有目标域样本特征的相似度，也是目标域样本属于源域类别的概率。最后我们去优化损失，这个损失主要的目的就是使熵值变小，换句话说就是使目标域的样本更确定自己属于已知类的某一类还是未知类的某一类，完成聚类任务。==？

## 4. 熵分离损失entropy separation(ES) loss

**假设：**

邻域聚类损失使目标域样本完成聚类，但我们仍然需要将其中一些目标样本与“已知”的源类别对齐，同时保持“未知”的目标样本远离源类别。[20.【2019】【UDA，UAN，提出新问题Universal DA，提出新网络UAN】【Mingsheng Long】Universal Domain Adaptation](./20.【2019】【UDA，UAN，提出新问题Universal DA，提出新网络UAN】【Mingsheng Long】Universal Domain Adaptation.md)说明了属于未知类的目标域样本比起已知类的目标域样本，其熵值可能更大。基于上面的假设，我们使用分类器输出的熵在“已知”和“未知”点之间画一个边界。

**符号：**

ρ表示阈值边界，ρ=log(K)/2这个阈值是根据直觉设置的，并不一定是这个阈值，其中K表示源域类别的个数，log(K)表示源域类别的最大熵。|H(p) - ρ|表示熵和阈值边界之间的距离，p是分类器的输出，我们假设“未知类”目标样本的熵将大于ρ，而“已知”目标样本的熵将小于ρ。m置信阈值参数，忽略一些模糊的点。

**损失函数：**

<img src="D:\markdown file\截图\image-20221107185211600.png" alt="image-20221107185211600" style="zoom:50%;" />

**具体流程：**

<img src="D:\markdown file\截图\image-20221107184725281.png" alt="image-20221107184725281" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221107185029644.png" alt="image-20221107185029644" style="zoom:50%;" />

首先进来一个目标域样本的预测值，我们先看熵和阈值边界之间的距离|H(p) - ρ|是否大于置信阈值参数m，如果小于m，我们就认为是一些模糊的点，损失置零略过，如果大于m，我们再带入计算。计算有两种可能，一种是H(p) > ρ，这种情况下，表示目标域样本预测的熵>阈值边界，根据上面的假设，我们就认为这是未知类，再带入损失函数，我们的目标是损失函数最小化，所以说|H(p) - ρ|越大越好，也就是说H(p)越大越好，如此以来，未知类离阈值边界就越来越远。还有一种可能是H(p) < ρ，这种情况下，表示目标域样本预测的熵<阈值边界，根据上面的假设，我们就认为这是已知类，再带入损失函数，我们的目标是损失函数最小化，所以说|H(p) - ρ|越大越好，也就是说H(p)越小越好，如此以来，已知类离阈值边界就越来越远。

**总损失：**L~cls~是普通的源域分类损失。

<img src="D:\markdown file\截图\image-20221107191306460.png" alt="image-20221107191306460" style="zoom:50%;" />

## 5. 创新点

设计了两个新损失：**邻域聚类neighborhood clustering和熵分离损失entropy separation loss。**

采用一个**基于中心的分类器prototype-based classifier**。

用到了**基于域的BN domain-specific batch normalization**。

==实际上作者的邻域聚类方法可以看作是下面两篇文章的结合。==

[34.【2018】【CVPR】【Zhirong Wu】【UC伯克利、香港中文大学】【引入一种新的无监督分类任务的方法】Unsupervised Feature Learning via Non-Parametric Instance Discrimination][./34.【2018】【CVPR】【Zhirong Wu】【UC伯克利、香港中文大学】【引入一种新的无监督分类任务的方法】Unsupervised Feature Learning via Non-Parametric Instance Discrimination.md]

[33.【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy](./33.【2019】【ICCV】【Kuniaki Saito】【波士顿大学、UC伯克利】【使用了最大化熵使类别原型wi向目标域无标签样本偏移】Semi-supervised Domain Adaptation via Minimax Entropy.md)

作者的储存器是：

<img src="D:\markdown file\截图\image-20221107152547880.png" alt="image-20221107152547880" style="zoom:50%;" />

可以看到其中储存器F前半部分储存的是每个实例的特征向量，这个思想在34这篇文章中用到，后面是类别原型向量(class prototype)这在文章33用到。

==其实这里已经有一点对比学习的意思了，是通过memory bank形式的对比学习，从后续的文章来看，这里是可以改进的，想MOCO那样==



