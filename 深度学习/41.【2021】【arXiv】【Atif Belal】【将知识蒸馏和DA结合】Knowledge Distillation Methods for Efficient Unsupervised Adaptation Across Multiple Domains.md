## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\41.【2021】【arXiv】【Atif Belal】【将知识蒸馏和DA结合】Knowledge Distillation Methods for Efficient Unsupervised Adaptation Across Multiple Domains.pdf)

### 1.1 在干什么、有什么贡献、什么结论

问题：同时使用模型压缩（就是知识蒸馏KD）和域适应DA的研究不多，所以我来做。之前的KD和DA结合主要用于减少域偏移，没有考虑用KD减少计算成本。

本文基于DA提出一种知识蒸馏KD的方法，可以同时减少计算成本，提高DA效果。并且可以扩展到多源域的DA问题。

中心思想是DA后，进行模型压缩。

<img src="D:\markdown file\截图\image-20221220185212612.png" alt="image-20221220185212612" style="zoom:50%;" />

上图是单源域DA和多源域DA用到的网络框架，以及作者加上KD后单源域DA和多源域DA用到的网络框架。

## 2. 网络结构

<img src="D:\markdown file\截图\image-20221221152336064.png" alt="image-20221221152336064" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221221152810485.png" alt="image-20221221152810485" style="zoom:50%;" />

一个是单目标域，一个是多目标域，实际差不多。