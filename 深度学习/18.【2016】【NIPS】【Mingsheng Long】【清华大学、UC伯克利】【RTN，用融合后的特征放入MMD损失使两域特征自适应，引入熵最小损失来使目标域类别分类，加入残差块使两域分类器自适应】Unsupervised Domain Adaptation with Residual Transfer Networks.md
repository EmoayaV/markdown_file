## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\18.【2016】【NIPS】【Mingsheng Long】【清华大学、UC伯克利】【RTN，用融合后的特征放入MMD损失使两域特征自适应，引入熵最小损失来使目标域类别分类，加入残差块使两域分类器自适应】Unsupervised Domain Adaptation with Residual Transfer Networks.pdf)

### 1.1 在干什么、有什么贡献、什么结论

[22.【2015】【arXiv】【Xu Zhang】【清华大学、哥伦比亚大学】【DTN，在第l-1层使用MMD损失，在最后一层产生伪标签来使用MMD损失】Deep Transfer Network Unsupervised Domain Adaptation](C:\Users\Emoaya\Desktop\文章\22.【2015】【arXiv】【Xu Zhang】【清华大学、哥伦比亚大学】【DTN，在第l-1层使用MMD损失，在最后一层产生伪标签来使用MMD损失】Deep Transfer Network Unsupervised Domain Adaptation.pdf)

**和上面文章的思想相同**，什么意思呢？就是说我们不光要拉近神经网络前L-1层提取出的特征的分布，还要拉近最后一层概率输出的分布，在下面的文章中神经网络前L-1层提取出的特征的分布叫做边缘分布，最后一层概率输出的分布叫做条件分布。

**背景：目标域无监督。**

以前的DA方法都是假设只要学到了域不变特征，那么源域的分类器就可以直接在目标域上使用。那么这个假设是对的吗？实际中两个域分布的差异是没办法消除的，就算经过神经网络提取出来的特征，其分布也有差异，这种差异只能减小，不能消除，并且也不好验证两个分别之间的差异消除了多少。

所以**作者就假设源域的分类器可能没法直接在目标域上使用**（比如源域和目标域的分布差异可能导致提取的特征和分类器的不匹配，这一点在[文章3](./3.【2014】【在不同的层数进行finetune和frozen看效果】【Yosinski、Bengio】How transferable are features in deep neural networks.md)中也有所提及，叫做层与层之间的特征的共适应性是脆弱的（*fragile co-adapted features* ）），如果直接使用可能有偏差，并受到残差网络的启发，提出一种**RTN的网络**可以同时学到**自适应的分类器**和**可迁移的特征**。

作者通过在深度网络中插入几层神经网络，来学习目标分类器的残差函数，以此实现分类器自适应的目的。

对于迁移特征，作者通过融合特征（就是计算神经网络中几个层的feature的张量积），把特征嵌入再生核希尔伯特空间（reproducing kernel Hilbert spaces），再去拟合源域和目标域的分布来实现。

## 2. 网络结构

![image-20221019152214741](D:\markdown file\截图\image-20221019152214741.png)

### 2.1 特征适应（迁移特征）

首先，CNNs之后加一个bottleneck layer（我查了一下，这个瓶颈层是为了减少特征的维度而被创建出来的） 用fcb来减少特征的维度。

之后使用经过fcb和fcc的特征，进行张量积操作，以此达到无损多层特征融合的目的，再把融合后的特征放入MMD损失函数（与DAN后3层放3个MMD损失不同，本文是把fcb层核fcc层输出的特征融合再放入MMD损失），最后在源域上微调（fine-tune） CNNs。目的是为了让source domain和target domain更加相似。

### 2.2 分类器适应

![image-20221019152214741](D:\markdown file\截图\image-20221019152214741.png)

**背景：**虽然已经进行特征适应（拉近源域数据特征和目标域数据特征的分布）但是这种不同域的特征分布的差异是不可能消除的，所以还要进行分类器适应。

假设源域的分类器fs(x)和目标域的分类器ft(x)差一个扰动（perturbation）Δf(x)，那么ft(x) = fs(x) + Δf(x)，但是只有在目标域有标签的情况下才可以学习这个扰动（**这个扰动我个人觉得是没有进行类别对齐**），所以对于无标签的目标域数据这个扰动没法学习到，虽然很难做到，但是作者还是假设可以通过源域有标签的数据和目标域无标签的数据学习到这个扰动。

![image-20221019173216280](D:\markdown file\截图\image-20221019173216280.png)

![image-20221019173234768](D:\markdown file\截图\image-20221019173234768.png)

**具体实现：**具体是通过残差块实现的，**首先把没经过激活层的fcc层的输出fT(x)作为目标域的预测结果（这里就是伪标签），再经过残差块网络后得到未经过激活层的输出fS(x)作为源域的预测结果。注意这个残差块的加法是按元素做加法**，不是concat操作。（作者在这里用fT(x)表示未经过激活层变为概率值的输出，fS(x)同理）

**为什么要选fcc层的输出作为目标域分类器的输出呢？为什么把经过残差块后的输出作为源域分类器的输出呢？**

作者的解释：我们将源分类器fS设置为残差块的输出，使其通过深度残差学习更好地从源标记数据中训练。如果将目标分类器fT设置为残差块的输出，我们可能无法成功地学习它，因为我们没有目标标记的数据，因此标准的反向传播将不起作用。

**后续：**虽然我们成功地将分类器自适应转换到残差学习框架中，残差学习框架倾向于使目标分类器ft(x)与源分类器fs(x)没有太大的偏差，但我们仍然不能保证ft(x)很好地适合目标域的分布。

**==用熵最小化来使类别低密度分离（类别对齐）：==**因此作者利用entropy minimization principle（**熵最小化原理**）来优化参数，通过最小化目标域各个类的条件分布的熵来鼓励目标域上类之间的低密度分离（low-density separation）我的理解是输出更加趋向一个one-hot vector，因为one-hot vector的熵是最低的，也就是说把不同类的特征分的更开一些，这样熵才会变小，比如[0.5, 0.5]熵=1[1, 0]熵=0。

**在目标域的伪标签上使用熵最小化：**



这个所谓的类条件分布（class-conditional distribution）就是目标分类器的输出，这个目标分类器的输出也叫做伪标签（pseudo label）。

**总的损失函数：**第一项代表只用源域数据训练的分类损失，第二项代表用于使目标域类别低密度分离的熵损失，第三项是使源域目标域分布拉近的MMD损失。

![image-20221019202203589](D:\markdown file\截图\image-20221019202203589.png)