## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\38.【2020】【arXiv】【Alexey Dosovitskiy】【谷歌】【提出Vision Transformer】An image is worth 16x16 words Transformers for image recognition at scale.pdf)

### 1.1 在干什么、有什么贡献、什么结论

我们直接把Transformer用在了图像分类任务上，为此我们将图片分割成一个个小块(patch)，将小块以序列的方式输入Transformer，因为Transformer缺少归纳偏置(inductive biases)，就是缺少一些先验概率，或者叫前提假设，所以在小数据集上(14M以下)性能不如CNN，但是在大数据集上(14M以上)性能比CNN好，因为网络自己学到了一种归纳偏置，比CNN人定的归纳偏置好。

## 2. 网络结构

<img src="D:\markdown file\截图\image-20221124130141873.png" alt="image-20221124130141873" style="zoom:50%;" />

<img src="D:\markdown file\截图\image-20221124131453092.png" alt="image-20221124131453092" style="zoom:50%;" />

首先将图片 x = [H, W, C] 分成一个个小块，再把每个小块展平为向量 x~p~ = [N, P^2^C]，其中N是patch的个数，[P, P] 是每个patch的长宽，最后把这组向量进行Embedding 编码+ 位置编码，得到Transformer Encoder的输入 x~p~E = [N, D]，如公式(1)所示。

![image-20221124132451102](D:\markdown file\截图\image-20221124132451102.png)

类似于BERT的类token，我们在输入序列的最前方加入一个token参与序列，其相应位置的Transformer Encoder输出最后经过一个MLP最终输出预测类别，完成分类任务，如公式(2)(3)(4)，其中MSA是指Multi-head Self-Attention。

我们使用1D的位置编码而不使用2D的位置编码，原因是使用2D位置编码并没有带来性能提升。

一般来说，我们在大型数据集上预先训练ViT，并对(较小的)下游任务进行微调，微调时要注意移除掉预训练好的MLP head，加入新的MLP head，输出的类别数根据下游任务而定，面对高分辨率图像，我们保持patch数不变，用更长的向量表示一个patch，这是VIT的正确用法。

