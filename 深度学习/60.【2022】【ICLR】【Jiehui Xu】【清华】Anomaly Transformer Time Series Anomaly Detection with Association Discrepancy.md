[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章_时序异常检测\6.【2022】【ICLR】【Jiehui Xu】【清华】Anomaly Transformer Time Series Anomaly Detection with Association Discrepancy.pdf)

## 1. 摘要abs和导言intro

问题：以前的方法主要是通过学习点表示或配对关联来解决问题，然而，这两种方法都不足以推理复杂的动态

解决：提出了一种新的Transformer（*Anomaly Transformer*）使用异常-注意（*Anomaly-Attention*）机制来计算关联差异。与传统的Transformer不同，异常Transformer（Anomaly Transformer）将自注意机制（self-attention）更新为基于关联差异关键观察的异常注意机制（Anomaly-Attention based on the key observation of association discrepancy）。（本质是基于重构的方法）

场景：无监督的时序异常检测。

实验：在6个无监督数据集上进行了评估。

#### 现有方法的不足：

基于深度学习的一类方法侧重于通过精心设计的循环网络学习点表示，并通过重建或自回归任务进行自我监督。一个自然而实用的异常判据是逐点重建或预测误差，然而，由于异常的罕见性，对于复杂的时间模式，逐点表示信息较少，使得异常难以区分。

## 2. 网络结构以及方法

<img src="D:\markdown file\截图\image-20221123164527968.png" alt="image-20221123164527968" style="zoom: 67%;" />

<img src="D:\markdown file\截图\image-20221123175505428.png" alt="image-20221123175505428" style="zoom:50%;" />

**Transformer**

![image-20230508094935053](D:\markdown file\截图\image-20230508094935053.png)

**作者提出的Transformer**

#### 模型细节

先验关联 prior-association：采用科学系的高斯核计算先验相对于相对时间距离。

时间序列关联 series-association：学习原始序列的关联。

异常注意力机制 Anomaly Attention：即原始的Transformer + 作者加入的高斯核

异常得分：使用Association Discrepancy作为异常得分，这个AssDic越小就证明是异常。

#### 异常得分

![image-20230509125023340](D:\markdown file\截图\image-20230509125023340.png)

![image-20230509125052003](D:\markdown file\截图\image-20230509125052003.png)

首先本文选取**Association Discrepancy**作为异常得分，**AssDic越小表明异常可能性越大**，AssDic中包括两部分KL散度（KL散度用来度量两个概率分布之间的差异程度,KL散度越大,说明两者的差异程度越大;当KL散度越小,则说明两者的差异程度也就越小）就是说当P(prior-association)和S(series-association)的分布越接近时，异常的可能性越大。这样就把异常问题转换为两个分布是否接近的问题，S是通过Transformer预测出的输出的分布，而**P是我们人为构造的分布，就是说我们通过P去人为定义了异常**，所以这个P才叫做先验关联。但是人为的不一定准确，所以加入了可学习参数σ。

![image-20230509135707462](D:\markdown file\截图\image-20230509135707462.png)

如果检测到异常，那么异常得分就会增高，那么增高由两个原因，要么AssDic太小了，要么重构误差太大了。这种设计可以使重构误差和关联差异协同工作，提高检测性能。



#### 模型训练

![image-20230509130330920](D:\markdown file\截图\image-20230509130330920.png)

![image-20230509131935660](D:\markdown file\截图\image-20230509131935660.png)

采用重构的方法，所以损失也是重构的MSE损失（L2损失）。后面将异常得分AssDic也纳入损失函数。

请注意，直接最大化关联差异将极大地降低高斯核的尺度参数，使先验关联变得毫无意义。简单来说就是直接最大化AssDic == 使S的分布和P的分布不一致 == P的分布方差越小那么与S的分布就越不一致（P是高斯分布，σ越大分布越胖，σ越小分布越瘦）== 使得P中的可学习参数σ变得非常小。

为了解决以上问题，作者采用了**最大最小化策略 a minimax strategy**（参考GAN训练 先固定generator参数，去训练discriminator参数。再固定discriminator参数，去训练generator参数，其实和这里的detach是一样的）：具体来说，对于最小化阶段**minimize phase**，我们将P分布去接近S的分布，即减小AssDic。在最大化阶段**maximize phase**，我们再拉开P分布和S分布的相似性，即扩大AssDic。（detach表示停止关联的梯度反向传播）



#### 代码部分的问题：

- 这个验证集的划分，有些直接把测试集当验证集。

![image-20230512170930032](D:\markdown file\截图\image-20230512170930032.png)

![image-20230512170956133](D:\markdown file\截图\image-20230512170956133.png)

- 窗口是100，位置编码取5000？这里需要改

![image-20230512171110519](D:\markdown file\截图\image-20230512171110519.png)











