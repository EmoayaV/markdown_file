## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\35.【2022】【arXiv】【Hangbo Bao】【微软】【提出BEiT，实际上就是VIT+类似于BERT的自监督训练】BEIT BERT Pre-Training of Image Transformers.pdf)

### 1.1 在干什么、有什么贡献、什么结论

我们提出了自监督的视觉模型BEiT，我们提供了一个自监督的任务，叫做遮盖图像模型masked image modeling (MIM)。

## 2. 网络模型

![image-20221123125136385](D:\markdown file\截图\image-20221123125136385.png)

上图是遮盖图像模型masked image modeling (MIM)的网络结构，主要用在BEiT的预训练阶段。首先将输入的图片分成图片块(image patches)，然后通过VAE的方法将这些图片块进行词源化(tokenize)的处理，于是我们就得到了一个视觉词源(visual tokens)，接着在BEiT的预训练阶段，我们将按照一定比例，随机的遮盖(masked)一张图片的一些图片块，再把有缺失的图片输入BEiT的Encoder里，将masked的图片块得到的输出hi再通过一个softmax的分类器，得到一个预测值，预测值的标签是原始图片经过tokenize后对应位置的token。对于下游任务(如图像分类和语义分割)，我们在预先训练的BEiT上附加任务层，并微调特定数据集上的参数。

**词源化细节：**

我们使用离散变分自编码器(dVAE)技术将图片词源化，在dVAE中有两个模块，分别是词源化器(tokenizer)和解码器(decoder)，词源化器将输入图片的像素根据图像词源字典(vocabulary)映射成词源(token)，解码器将词源还原成原始输入图片。有一个细节是作者用的图像词源字典总共由8192个词源(token)，这类似于字典中的单词个数。

**骨干网络：Transformer的Encoder**

和Transformer相同，先将输入的图片块进行Embedding操作，加上位置编码，在开头加一个向量表示类别。