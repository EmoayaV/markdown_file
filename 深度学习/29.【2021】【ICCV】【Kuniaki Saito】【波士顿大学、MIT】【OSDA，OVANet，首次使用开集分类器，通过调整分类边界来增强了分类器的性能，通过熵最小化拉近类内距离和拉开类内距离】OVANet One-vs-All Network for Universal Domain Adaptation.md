## 1. 摘要abs和导言intro

[文章链接](C:\Users\Emoaya\Desktop\文章\精读文章\29.【2021】【ICCV】【Kuniaki Saito】【波士顿大学、MIT】【OSDA，OVANet，首次使用开集分类器，通过调整分类边界来增强了分类器的性能，通过熵最小化拉近类内距离和拉开类内距离】OVANet One-vs-All Network for Universal Domain Adaptation.pdf)

### 1.1 在干什么、有什么贡献、什么结论

<img src="D:\markdown file\截图\image-20221114162653816.png" alt="image-20221114162653816" style="zoom:67%;" />

提出了One Vs All Net (OVANet)，如上图所示：

我们假设源域的**最小类间距离是确定样本是否来自类的一个很好的阈值来决定目标中的“已知”或“未知”。**因为它定义了来自其他类的最小距离。如果样本与类之间的距离小于差值，则该样本应该属于该类。如果样本不在任何类的空白范围内，那么它应该是未知的。

为了学习类间和类内的距离，使用带标签的源数据为源域中的每个类训练一个一对多 (one vs all) 的分类器。分类器学习到正类和负类之间的界限，如果所有的分类器都认为输入是负的，我们假设输入来自未知的类。

## 2.网络结构

<img src="D:\markdown file\截图\image-20221114180311315.png" alt="image-20221114180311315" style="zoom:50%;" />

![image-20221114183817953](D:\markdown file\截图\image-20221114183817953.png)

为了处理 open set 的分类任务，我们引入了 (open set classifier) 开集分类器O和 (closed set classifier) 闭集分类器C，C用源域样本训练，用于对已知类分类，O用不易分类的负样本训练 ( Hard Negative Sample，不同但类似的类) ，用于判断已知类还是未知类。

**作者的方法有三个部分，分别对应网络的上中下三个部分。**

**open set classifier training 部分：**

这部分通过输入源域的样本来训练开集分类器O。

**开集分类器O由Ls个二分类器组成，其中Ls是源域类别的个数，每个二分类器都经过训练，以区分样本是否为对应类。二分类器输出一个二维向量，其中每个维度分别显示样本是否为对应的概率。**

**注意这里一定是用不易分类的负样本训练，不是全部的样本去训练分类边界。**

作者举了一个例子，比如有三个类，乌龟 ，猫和狗，假设乌龟的特征与猫的特征距离很远，狗的特征与猫的特征距离很进，如果我们想学习一个关于猫类别的分类边界，那么我们应该着重区分猫和狗的特征而不是猫和乌龟的特征，如果着重于猫和乌龟的特征，我们就会在猫和乌龟的中间学到一个分类边界，这样会把大量的其他类判为猫，这样不行，所以一定要用不易分类的负样本进行训练，有点像SVM中的支持向量。

基于上面的想法，我们通过为每个源域样本选择一个最近的负类，我们可以让相应的分类器学习一个有效的边界来识别未知实例。如下图所示：

<img src="D:\markdown file\截图\image-20221114165302015.png" alt="image-20221114165302015" style="zoom:50%;" />

我们通过下面的损失来得到样本最近的负类，损失如下所示：

<img src="D:\markdown file\截图\image-20221114180607435.png" alt="image-20221114180607435" style="zoom: 67%;" />

==这里后半部分的损失看不懂==

其中前半部分的损失保证对样本xs正确分类，后半部分的损失是为了找到离样本xs最像的负类。

上面整个过程也称作 Hard Negative Classifier Sampling (HNCS) 

**open set entropy minimization 部分：** 

这部分通过输入目标域的样本来训练开集分类器O。

我们计算所有分类器的熵，取平均值，训练一个最小化熵的模型，通过熵最小化，拉近类内距离和拉开类内距离，已知类的目标样本将与源样本对齐，而未知类的样本将保持为未知。

与现有的熵最小化的一个明显区别是，我们能够保持未知实例为未知，因为熵最小化是由开集分类器执行的，而不是由闭集分类器执行的。闭集分类器的熵最小化必然使未标记的样本与已知的类对齐，因为没有未知类的概念。由于我们的开放集分类器具有未知的概念，该模型可以增加未知的置信度。

具体来说是上面意思呢？假如源域有K类，以前的分类器会输出K个概率，如果进行熵最小化，那么对于未知类就会强行对齐到K中的一个类别，而作者的分类器则不同，作者用了K个二分类器来完成分类任务，会输出2K个类别，如果进行熵最小化，那么对于未知类就不会强行对齐到这K个已知类别上。举个例子，三类狗猫乌龟，原始的闭集分类器就输出三个概率，如果出现第四个未知类兔子，那么使用熵最小化，就会使未知类兔子最大的预测概率强行对齐到是狗、是猫、是乌龟这三类之一。而使用开集分类器就会输出六个概率 (是狗、是猫、是乌龟、不是狗、不是猫和不是乌龟) ，使用熵最小化就会使未知类兔子对齐到不是狗、不是猫和不是乌龟这三类之一。

损失函数就是一个信息熵的计算公式，如下图所示：

<img src="D:\markdown file\截图\image-20221114190731784.png" alt="image-20221114190731784" style="zoom:67%;" />

**deployment 部分：**

在前面的基础上，我们结合开集分类器和闭集分类器来学习开集和闭集分类。对于闭集分类器，我们使用交叉熵损失在特征提取器之上训练一个分类器，我们用L~cls~(x, y)表示，那么总的损失如下所示：

<img src="D:\markdown file\截图\image-20221114191349140.png" alt="image-20221114191349140" style="zoom:67%;" />

在测试阶段，我们同时使用训练好的闭集和开集分类器。我们首先用闭集分类器得到最接近的已知类，然后取开集分类器的相应分数。

## 创新点

==1.不同于其他文章，本文在第一部分主要是**通过调整分类边界来增强了分类器的性能**，这一点很新颖。不光在DA，在进行其他分类任务时也可以用这个思想。==

==2.**闭集分类器**这个概念是第一次使用，这种闭集分类器的分类边界应该比以往的开集分类器的分类边界要优秀很多，这一点也是很新的，可以用在其他领域。从直觉上来说，如果闭集分类器输出K个概率，那么开集分类器会输出2K个概率，信息量也更大了，有益于分类任务，同时也对分类任务提出更高的标准，假设5分类任务，以往的分类任务会有5个输出，前4类都预测概率不高，那么肯定就是第5类了，现在的分类任务有10个输出，如果前4类概率不高，也不一定是第5类，形象点说就是选择题和填空题的区别。==

==3.这里的第二部分，**通过熵最小化拉近类内距离和拉开类内距离**，实际上在其他文献中也出现过，比如下面的文献，作者这里的创新点是在开集分类器上使熵最小化，而其他文章只在闭集分类器上使熵最小化。==

[29.【2021】【ICCV】【Kuniaki Saito】【波士顿大学、MIT】【OSDA，OVANet，首次使用开集分类器，通过调整分类边界来增强了分类器的性能，通过熵最小化拉近类内距离和拉开类内距离】OVANet One-vs-All Network for Universal Domain Adaptation](./29.【2021】【ICCV】【Kuniaki Saito】【波士顿大学、MIT】【OSDA，OVANet，首次使用开集分类器，通过调整分类边界来增强了分类器的性能，通过熵最小化拉近类内距离和拉开类内距离】OVANet One-vs-All Network for Universal Domain Adaptation.md)

>就是本文，通过在开集分类器上使用熵最小化来拉近类内距离，拉开类间距离

[28.【2020】【ICML】【Yadan Luo】【OSDA，提出PGL方法，使用GNN网络完成DA任务，将有伪标签的目标数据加到源数据迭代训练】Progressive Graph Learning for Open-Set Domain Adaptation](./28.【2020】【ICML】【Yadan Luo】【OSDA，提出PGL方法，使用GNN网络完成DA任务，将有伪标签的目标数据加到源数据迭代训练】Progressive Graph Learning for Open-Set Domain Adaptation.md)

>使用伪标签来拉近类内距离，拉开类间距离

[27.【2020】【ECCV】【Silvia Bucci】【意大利理工学院】【OSDA，提出ROS两阶段方法，将自监督学习融入DA任务中】On the Effectiveness of Image Rotation for Open Set Domain Adaptation](./27.【2020】【ECCV】【Silvia Bucci】【意大利理工学院】【OSDA，提出ROS两阶段方法，将自监督学习融入DA任务中】On the Effectiveness of Image Rotation for Open Set Domain Adaptation.md)

>使用自监督来拉近类内距离，拉开类间距离

[23.【2020】【NIPS】【Kuniaki Saito】【波士顿大学、MIT】【UDA，引入熵分离损失和邻域聚类损失】Universal Domain Adaptation through Self-Supervision](./23.【2020】【NIPS】【Kuniaki Saito】【波士顿大学、MIT】【UDA，引入熵分离损失和邻域聚类损失】Universal Domain Adaptation through Self-Supervision.md)

>用熵和领域聚类的方法来拉近类内距离，拉开类间距离

[10.【2017】【ICCV】【Saeid Motiian】【西弗吉尼亚大学】【给出了一种基于线性距离度量和相似性度量的损失减小域之间的差异】Unified Deep Supervised Domain Adaptation and Generalization](./10.【2017】【ICCV】【Saeid Motiian】【西弗吉尼亚大学】【给出了一种基于线性距离度量和相似性度量的损失减小域之间的差异】Unified Deep Supervised Domain Adaptation and Generalization.md)

>用度量学习的方式来拉近类内距离，拉开类间距离

[6.【2015】【ICCV】【Eric Tzeng、Hoffman】【UC伯克利】【用对抗对齐特征用软标签对齐类别】Simultaneous Deep Transfer Across Domains and Tasks](./6.【2015】【ICCV】【Eric Tzeng、Hoffman】【UC伯克利】【用对抗对齐特征用软标签对齐类别】Simultaneous Deep Transfer Across Domains and Tasks.md)

>用软标签的方式来拉近类内距离，拉开类间距离

